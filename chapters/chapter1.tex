\documentclass[thesis.tex]{subfiles}

\begin{document}

\section*{Overview} 
\epigraph{``Maybe the universe isn’t total chaos.''}{Annie Landsberg \\ Maniac, S1 E10: `Option C' (2018)}
% \Gls{latex} \gls{latex} \gls{lambda} $\theta = \gls{lambda}$ % LINDEN UNCOMMENT
Advances in technology in recent decades have enabled both the generation and computational analysis of genomic data on unprecedented scales, transforming research and clinical practice throughout medicine in the process.
This effect has been especially potent in fields with a strong molecular biology component, such as oncology. While cheap abundant biological data has unlocked invaluable new insights, it has demanded complementary advances in statistical and machine learning in order to answer theoretical and practical questions. The modern researcher has access to a catalogue of tools, including methods inspired by progress in AI from disciplines such as natural language processing and image analysis. However, before employing the latest and greatest technique 
from predictive or generative modelling, it is worth asking a sequence of questions. What sort of data are we dealing with in genomic medicine? Given the abundance of data available for a particular problem, what level of model complexity is appropriate? If our methods do work, will we understand why? What stage of the clinical pipeline is being targeted? If our hope is for the deployment of predictive systems in clinical settings, are our tools robust enough? If so, are the technologies upon which they rely economically viable? While we will certainly not answer all of these questions, in the chapters that follow we will provide some language with which to discuss them, and a range of application cases across genomic medicine. 

This introduction should be equally approachable to those with a background in statistics/machine learning and those from biology. We'll begin by providing context for genomics research as the starting point for much of modern medicine, including the discovery of diseases mechanisms, biomarkers, and therapeutics. We'll then discuss the role of predictive models in enabling `precision medicine', and their substantial challenges. In a complementary strand, we'll describe how the language of statistical learning can be used to phrase and interrogate biological questions, as well as those arising from modern developments in machine learning. After an introduction to the types of data encountered in sequencing-based studies along with the opportunities and problems they present, we'll provide some terminology and useful concepts from high-dimensional statistics, Bayesian statistics, and causal inference, and will discuss how these concepts arise naturally in the context of genomic medicine. This will be accompanied by some illustrative examples of how different techniques may be employed in translational scientific research, but the bulk of description of specific applications is left to the following chapters. We will conclude with a summary of the application cases considered throughout the rest of this thesis.

\subsection{Genomics in medicine}
Since the success of the Human Genome Project \citep{lander_initial_2001}, sequencing technologies
have improved at an exponential rate, both in terms of cost per megabase
sequenced \citep{wetterstrand_dna_2022} and the number of individuals who have had some portion of
their genome sequenced. Advances have also been made in accuracy and the capacity for long-read sequencing \citep{goldfeder_human_2017}. This has introduced an invaluable new resource
into biomedical research, and initiatives such as the 10,000/100,000 Genomes Projects \citep{telenti_deep_2016} and the UK Biobank \citep{bycroft_uk_2018} have drastically improved the accessibility and infrastructure surrounding this data \citep{szustakowski_advancing_2021}. This is beginning to show impact in clinical settings \citep{prokop_genome_2018}. 

In the study of cancer, a disease of the
genome, the ability to rapidly and cheaply sequence normal and tumour-derived \glsxtrshort{dna} has transformed basic research, birthing the field of cancer
genomics. While \glsxtrlong{wgs} is not yet {standard-of-care} for the generic cancer patient,
access to in-depth genetic data is becoming more common. Cancer-specific repositories such as The Cancer Genome Atlas \citep{weinstein_cancer_2013} have given
researchers access to large clinical datasets with a variety of accompanying
genomics information, including somatic mutations, copy number alternations, and gene expression profiles. 

The clinical impacts of genomic data have manifested themselves in many ways. One use of genomic data in a clincal setting is for subtyping/endotyping patients in a way that informs treatment decisions -- a key tenet of so-called `personalised medicine' or `precision medicine'. This may involve categorisation in very broad terms, such as the separation of virally and bacterially infected patients (see Chapter~\ref{chap:lamp_modelling} and \citealp{remmel_diagnostic_2022}), or within cancer distinguishing tumours according to the molecular profile of their mutations \citep{zhao_molecular_2019}.

Understanding the genomic landscape of disease is also critical to the field of early-stage drug discovery \citep{nelson_support_2015, raja_integrating_2017, king_are_2019}. In cancer, knowledge of the location and associated products of oncogenes (genes in which mutation can cause a cell to become cancerous) can allow for intelligent selection of druggable sites \citep{weinstein_cancer_2002, bedard_small_2020}, and identification of tumour suppressor genes (genes which under normal circumstances prevent uncontrolled cell division) gives options for therapies which may replace cancer patients' defective cell cycle control mechanisms \citep{fang_tumor-suppressing_2003, morris_therapeutic_2015}. 

Alongside new drugs, it has become increasingly common for therapies to be offered alongisde genomic biomarkers which may stratify patients who are more likely to benefit from the treatment \citep{weber_egfr_2014, awad_precision_2019, zhu_association_2019, safarika_29-mrna_2021}. This is important for a variety of reasons: to prevent unnecessary suffering for patients unlikely to benefit from drugs with adverse side-effects; to minimise unnecessary cost of wasted treatments unlikely to succeed; and to allow management of the use of therapies for reasons such as antimicrobial stewardship.

New sources and modalities of data have allowed researchers a greatly expanded
toolbox with which to investigate the causes and development of cancer and other diseases. Addressing these goals, however, presents a unique set of challenges. Firstly, in clinical settings it is commonly necessary (or at least very useful) to have some sense of the uncertainty accompanying any given prediction or assignment. Secondly, the number of covariates common in ’omics datasets induces a variety of theoretical and practical problems for classical statistical analysis, a problem often referred to as the curse of dimensionality \citep{barbour_precision_2019, buhlmann_high-dimensional_2014}. Finally, genomic datasets are often collected in observational settings, with limited interventional control, incomplete observation, and compiled from multiple heterogeneous data sources. In later sections we introduce some of the methods developed to address each of these methodological challenges.

\subsection{Statistical learning and machine learning} 
% This thesis will, for a set of specific application cases, address some of the questions raised in the previous section, with the aim of making progress towards enabling personalised and predictive genomic medicine. To do so, we'll use tools and methods from statistical learning and machine learning, so we begin here with a brief introduction to these fields, with further elaboration in Section~\ref{sec:statistical_learning}. 
Informally, statistical learning and machine learning attempt to address the
theoretical and practical challenges associated with extracting information from data by `fitting' (or sometimes `training') models that can be used to achieve some goal.  Often, this goal is to use a fitted model by predicting a future outcome for some new data points. This is known as \emph{regression} when predicting continuous outcomes, and \emph{classification} when predicting discrete outcomes. In this work we are principally concerned with predictive modelling. While we might be interested in the details of a model we've fitted (we'll  refer to this as `inference'), its subsequent application is generally the primary goal.

The distinction between machine learning and statistical learning is vague and constantly evolving. 
Several trends are informative but not prescriptive in describing statistical vs machine learning, including an emphasis in statistical learning on theoretical and mathematical guarantees on the future performance of predictive models, a tendency towards scale and non-linearity as features of machine learning models, and (particularly more recently) a specific focus in machine learning on neural networks and deep learning. Despite their differences in nomenclature and focus, however, statistical and machine learning share many common themes and goals. Most notably, they are both concerned with extracting useful information from data as efficiently and robustly as possible. In general, throughout the rest of this work we'll use the term statistical learning.

In order to produce performant predictive systems, extensive research exists towards understanding how best to utilise the \emph{structure} of the complex data types that underlie many modern common prediction problems. Examples of data types that have seen intense (sometimes frenzied) research over the last decade include images, text, and (as in our case) high-throughput genomics. Notably, none of these data types necessarily follows a standard tabular structure, and all are to some degree high-dimensional (i.e. contain a lot of information per input observation). There is currently a great deal of debate around the role of structure-informed learning algorithms in the fields of \gls{ir} and \gls{nlp}. Historically, huge breakthroughs have come from developing learning algorithms that `exploited the structure' of each of these data types, including convolutional neural networks for \gls{ir} \citep{krizhevsky_imagenet_2012, he_deep_2016, huang_densely_2018} and word embeddings for \gls{nlp} \citep{mikolov_efficient_2013, bojanowski_enriching_2017, almeida_word_2019}. However, recent years have seen a trajectory tending towards a relatively small set of model architectures dominating across surprisingly multi-disciplinary divides \citep{vaswani_attention_2017, dosovitskiy_image_2021}, with the role of inductive bias and natural complexity regularisation speculated to contribute to this success \citep{de_palma_random_2019, valle-perez_deep_2019, goldblum_no_2023}.

It has long been hoped that similar strides forward can be anticipated in biomedical science, but even with the context as laid out above it certainly remains unclear what strategies will work best for genomic and multi-omic data. A few key properties of genomic data distinguish it from other media. Firstly, its dimensionality: while images and text documents can be large, they do not come close to the scale of the human genome. The dimensionality of genomic data depends greatly on the technology being used and and with what `resolution' the data is being viewed. For example, in Chapters~\ref{chap:tmb_estimation}~and~\ref{chap:causal_genomics} we take a `gene-level' view of somatic mutation data, giving a dimensionality in the tens of thousands, while taken at its maximum dimensionality \gls{wgs} data includes measurements of billions of individual nucleotide bases. We'll discuss choices around technology and data dimensionality more in Sections~\ref{sec:sequencing}~and~\ref{sec:questions}. Secondly, data availability: as mentioned, human genomics data has exploded in abundance over the last two decades, but faces some fairly fundamental limits. Put simply, even with ultra-pervasive sequencing, it seems likely that most humans will produce far more text and images across their lifetime than they will novel genomes for sequencing\footnote{Note however that this may not be as obvious as it first seems, as advances in techniques for profiling genomic heterogeneity advance even to single-cell resolution.}. Finally, prediction of emergent phenotypes from genomic data may in general simply be more complex than image and text prediction/generation tasks. Image- and text-based tasks are, by design, typically achievable by humans, and these data types are highly compressible. There is, however, no requirement for evolution to have produced a dependence structure within genomic data that is so latently simple. For all of these reasons, there is no guarantee that approaches that have borne fruit in other disciplines will necessarily do so in genomic medicine. Even if they do, we may yet be far from the required richness of data availability to make full use of these methods.
Given these potential constraints, there has certainly been a great deal of work towards applying methods from the statistical and machine learning toolkit to genomics, including but not limited to random forests \citep{breiman_random_2001, chen_random_2012}, \glspl{svm} \citep{cortes_support-vector_1995, huang_applications_2018}, and \glspl{ann} \citep{avsec_effective_2021, tran_deep_2021}. 
 
In each of the chapters that follow we will have to make choices about how to make best use of the structure of the data types presented, including functional reaction-curve data from gene expression assays in Chapter~\ref{chap:lamp_modelling}, exome-wide somatic mutation data in Chapter~\ref{chap:tmb_estimation}, and heterogeneous multi-study survival data in Chapter~\ref{chap:causal_genomics}. 

% That is not to say that we can't do anything! In fact, it is often instructive to try and make headway in situations where a `data-heavy, structure-light' approach is unsuitable, and these sorts of investigations can have a profound impact on the design of more sophisticated models \citep{buhlmann_high-dimensional_2014}. As a final point, readers approaching without a significant backlog of machine learning expertise will find that an understanding of statistical terminology will aid comprehension of the machine learning literature which has them as its basis.

\section{Genomics and biological data}
\epigraph{``En particulier, on ne
saurait préciser actuellement le mécanisme selon lequel
les gènes désoxyribonucléiques peuvent commander l'édification des acides ribonucléiques cytoplasmiques et le
mécanisme selon lequel ces acides ribonucléiques
peuvent présider à l'élaboration des enzymes...''}{\citet{boivin_sur_1947}}

In this section we'll review the central tenets of molecular biology necessary to understand the rest of this work. In particular, we'll focus on how information flows between the three classes of molecule whose interactions determine the majority of cellular functionality. These are \glsxtrshort{dna}, \glsxtrshort{rna}, and proteins, and the flow of information between them is so consistent across nature that it is referred to as the `central dogma' of molecular biology. Describing this process also allows us to exhibit the main new sources of data that have enabled the genomics (or more generally `omics', to distinguish from \glsxtrshort{dna}-only analysis) revolution, and to give context to their respective uses and limitations. 

Once we've reviewed the molecular biology necessary for subsequent chapters, we'll discuss the specific role of genomics data in medicine and the study of disease. Because cancer forms the dominant thread of the second half of this thesis, we'll devote extra time to discuss some fundamental concepts from cancer genomics. We'll then go into more detail in the specifics of modern technologies for sequencing and quantification across major genomics data types.

\subsection{The central dogma \label{sec:central_dogma}}
Molecular biology has historically enabled the successful collision of two major subfields of biology concerning evolution and cells respectively. On the one hand, evolutionary biology has concerned the role of inheritance and diversity in producing the range of organisms observed in the natural world. On the other, cellular biology has concerned the mechanistic operation of cells, the constituent units of all living beings. Molecular biology provides the link between these two disparate areas of study via the flow of information between two classes of molecule. Firstly, \glsxtrshort{dna} is the carrier of genetic information and provides the mechanism for Mendelian and Darwinian inheritance. Secondly, proteins are the machines that, at the lowest level of resolution, perform the tasks that sustain and manage the operation of cells in sickness and in health. 

We now have a fairly nuanced understand of the connections between the two, with \glsxtrshort{rna} sat between them in the role of `messenger'\footnote{In reality, \glsxtrshort{rna} does a lot more than just this. As far as the central dogma goes, however, this is \glsxtrshort{rna}'s main function.}. Molecular biology, via a number of key results including the discovery of the structure of \glsxtrshort{dna} in the 1950s by Crick, Franklin, Watson and Wilkins \citep{franklin_molecular_1953, watson_molecular_1953, wilkins_molecular_1953, maddox_double_2003}, the role of \glsxtrshort{rna} as an intermediary between \glsxtrshort{dna} and proteins \citep{boivin_sur_1947, crick_protein_1958, brenner_unstable_1961, cobb_who_2015}, and the `cracking' of the genetic code \citep{crick_general_1961, nirenberg_rna_1964, brenner_uga_1967, tamura_genetic_2016}, enabled a conceptual and practical bridge
between disciplines and a fundamental unification of modern biology. 

The central dogma, a term coined by Crick, can be described as follows (see also Figure~\ref{fig:central_dogma}). Contained within the nucleotide sequence of \glsxtrshort{dna} -- often represented as a string of letters $A$, $C$, $G$ and $T$,  representing the nucleotide bases adenine, cytosine, guanine and thymine respectively -- is the hereditary information of an organism. As is necessary for replication at the cellular or organismal level, \glsxtrshort{dna} can be copied with the help of an enzyme known as a \glsxtrshort{dna} polymerase. This is represented by a cyclic arrow from \glsxtrshort{dna} to itself in Figure~\ref{fig:central_dogma}. With the aid of an enzyme known as an \glsxtrshort{rna} polymerase, \gls{mrna} molecules can be can be `transcribed' from a \glsxtrshort{dna} sequence. This leaves the \glsxtrshort{dna} molecule unchanged in its nucleotide information content but produces a single-stranded \gls{mrna} molecule that may be transported out of the cell nucleus where \glsxtrshort{dna} is typically stored. The word `transcription' is used to emphasise that, while chemically slightly different, \glsxtrshort{dna} and \glsxtrshort{rna} are essentially expressing the same language, with the nucleotides $A$/$C$/$G$/$T$ in \glsxtrshort{dna} directly mapping to the nucleotides $A$/$C$/$G$/$U$ in \glsxtrshort{rna} (the letter U in represents the nucleotide uracil, itself only differing from thymine by a single methyl group). Like \glsxtrshort{dna}, \glsxtrshort{rna} may also be used as a template for direct replication with the aid of an \glsxtrshort{rna} replicase enzyme, also known as \gls{rdrp}.

\begin{figure}[htbp]
\centering
\begin{tikzcd}[row sep = small, column sep = large]
DNA \arrow[loop above] \arrow[r, "transciption"] & RNA \arrow[loop above] \arrow[r, "translation"]  \arrow[l, bend left, dashed, "reverse \ transcription"] & Protein \\
& & \\
(Data) & (Message) & (Action) 
\end{tikzcd}
\caption{The central dogma.\label{fig:central_dogma}}
\end{figure}

For a typical gene, \glsxtrshort{rna} molecules are transported to the cytoplasm of the cell and, in structures known as ribosomes -- themselves built out of a mixture of \gls{rrna} and protein --, are \emph{translated} to proteins. Proteins are composed of sequences of amino acids, of which there are twenty types. Each amino acid is associated with a set of three-nucleotide \emph{codons}, with special codons also associated with starting and stopping translation. Together, this collection of codon-to-protein instruction maps are referred to as the `genetic code'.

The central dogma refers to the one-directional flow of information along the \glsxtrshort{dna} $\rightarrow$ \gls{mrna} $\rightarrow$ Protein chain. Crucially, while genetic information contained in nucleic acids can be converted to protein via translation of \gls{mrna}, the reverse will never occur. It was historically unclear whether the one-directional flow of information would extend to transcription of \glsxtrshort{dna} to \glsxtrshort{rna}. This was settled when it was discovered that the conversion of \glsxtrshort{rna}  to \glsxtrshort{dna}  via reverse transcription, while rare in nature, is demonstrated by various retroviruses \citep{baltimore_rna-dependent_1970, temin_rna-dependent_1970, coffin_discovery_2016}. Nowadays, reverse transcription technology  is commonly used throughout genomic research, for example as part of sample preparation for the clinical gene expression testing approach described in Chapter~\ref{chap:lamp_modelling}. This test also makes extensive use of \glsxtrshort{dna} polymerisation at each reaction stage.

The three classes of molecule described by the central dogma are also the basis of the main new data sources that have powered the genomics revolution of the last two decades. Advances in \glsxtrshort{dna} sequencing have allowed systematic and high-throughput analyses of the relationships between traits (phenotype) and \glsxtrshort{dna} sequence (genotype). Where in previous decades slow and labour-intensive procedures such as Sanger sequencing \citep{sanger_dna_1977} would have been required for any direct analysis of nucleotide sequence, today we have massively parallelised and automated systems for achieving the same analysis at far lower cost (we'll discuss these more in Section~\ref{sec:sequencing}). In Chapters~\ref{chap:tmb_estimation} and \ref{chap:causal_genomics} we'll be working with \glsxtrshort{dna} sequencing data derived from cancer patients. In fact, for each patient in the studies we discuss we'll be considering \emph{two} sets of \glsxtrshort{dna} sequencing data -- one from the patient's `normal' cells, and the other from cancerous tumour cells. We'll be particularly concerned with the points at which these two sequences differ.

The advances in sequencing described above have, in turn, also revolutionised \glsxtrshort{rna} analysis, birthing the field of transcriptomics. In both genomics and transcriptomics, however, high-throughput techniques have not entirely replaced more direct approaches to detection and quantification of \glsxtrshort{dna}/\glsxtrshort{rna}. During the COVID-19 pandemic, for example, extensive use was made of diagnostic tests based on \gls{pcr} to detect the \glsxtrshort{rna} sequence of the \gls{sarscov2} pathogen. For this application, sequencing would have been unnecessarily complex in order to detect and quantify a single \glsxtrshort{rna} sequence. In Chapter~\ref{chap:lamp_modelling}, we'll see another application of a direct quantification technique with many similarities to \gls{pcr} to a `medium-throughput' problem, in which for different reasons both sequencing and \gls{pcr} are unsatisfactory.

Advances to medium- and high-throughput study of protein abundance in cells have also been made (referred to as proteomics), but have typically relied on different underlying techniques to genomics/transcriptomics. While changes may come with the advent of exciting new nanopore-based single-molcule protein sequencing \citep{afshar_bakshloo_nanopore-based_2022, motone_not_2023}, this is as of yet far from commonplace. While proteomic data is certainly valuable as a snapshot of the functional state of a given sample, we don't consider proteomic data in any detail in this work precisely due to its relative lack of abundance. 


\subsection{Genomics in disease}
While interest in the patterns of inheritance of common and rare diseases predates our understanding of genetics or even Mendelian inheritance \citep{emery_joseph_1989}, the advent of molecular biology transformed our understanding not just of patterns of disease inheritance, but also in some cases directly of the mechanisms of disease. Early examples of conditions with genetically identified causes included sickle-cell anaemia \citep{ingram_specific_1956} and Down Syndrome \citep{lejeune_human_1959}. Notably, historic advances in our understanding of disease have often led to improvements in prognosis and diagnosis, but far less frequently to successful treatments. For example, as early as 1961 Robert Guthrie developed a method for screening infants for \gls{pku} that depended on detection of an abnormal amino acid (whose role in \gls{pku} was itself discovered nearly thirty years earlier; \citealp{folling_uber_1934}). While treatable with dietary changes, however, \gls{pku} is not directly curable to this day \citep{mohanty_century_2014}. Similarly, while recent advances in gene therapy have sparked hope for a general-purpose cure to sickle-cell anaemia (currently a blood and bone marrow transplant from a close genetic match is required), this still seems some way off, re-emphasising the typically long arc from genetics to cure.

The inherent difficulty in translating advances in basic science to treatment and drug discovery has persisted into the genomics era, perhaps more so than was envisioned by many at the conclusion of the Human Genome Project \citep[see, for example,][]{emilien_impact_2000}. In particular, end-to-end success rates for clinical trials have not improved substantially \citep{dowden_trends_2019}, although there is mounting retrospective evidence that support from genomic data is a major predictor of clinical trial success \citep{nelson_support_2015, king_are_2019}.

Outside of therapeutic and pharmacological spheres, however, there has been substantial progress in the development and deployment of genomics-based approaches to guide clinical decision making or track the course of disease. These have included the use of genomic data in diagnostic tests and as accompanying biomarkers to therapeutics. Here we'll discuss an example of each, which (conveniently) correspond to the subjects of Chapters~\ref{chap:lamp_modelling}~and~\ref{chap:tmb_estimation} respectively. 

Chapter~\ref{chap:lamp_modelling} is placed in the context of acute care patients who may experience inflammation and/or sepsis/septic shock. Systemic inflammation (the erroneous or excessive application of the process by which the body fights infection) can be caused by a multitude of factors, including bacterial infection, viral infection, and non-disease related mechanism such as response to trauma \citep{chen_inflammatory_2017}. Appropriate clinical action depends upon determining the origin of inflammation. This is particularly true when deciding whether to treat with antibodies, whose non-use can entail significant and imminent impacts to mortality \citep{liu_timing_2017} but whose inappropriate use can contribute to the development of antimicrobial resistance \citep{fitzpatrick_sepsis_2019}. Accurate diagnostic testing for bacterial infection is therefore crucial. Genomic technologies have been used to test for a multitude of bacterial infections, often through the detection of specific \glsxtrshort{dna}/\glsxtrshort{rna} sequences \citep{fournier_clinical_2014}. However, the general success of this strategy relies upon any given bacterial infection falling within the set of genomic targets of a given detection assay. There has recently been interest (and a certain degree of success) in developing genomics-based diagnostic tests based on indirect detection via profiling \emph{host response} rather than directly via detection of a pathogen \citep{safarika_29-mrna_2021, kelly_systematic_2022}. While pathogen detection tests are based on a set of targets (often \glsxtrshort{dna} targets for bacterial infections) from the pathogens in question, host response diagnostic tests will typically comprise a set of human gene targets and will be based on profiling of gene expression \citep{ram-mohan_using_2022} or protein expression \citep{vanderboom_proteomic_2021}.

Chapter~\ref{chap:tmb_estimation} concerns the estimation of high-throughput genomic biomarkers. Very generally, a \emph{biomarker} is a quantity that can be measured for a given patient or sample and provides some relevant clinical information. Different biomarkers target very different characteristics, for example by measuring the abundance of a given chemical (e.g.~a small molecule; \citealp{qiu_small_2023}, or a large protein such as \gls{pdl1}; \citealp{doroshow_pd-l1_2021}), the visual characteristics of a sample \citep{smith_biomarkers_2003}, or the mutation status of a given genetic locus or gene (e.g. BRCA biomarkers in breast cancer; \citealp{walsh_genomic_2016}). As well as measuring a wide variety of phenomena, biomarkers are also diverse in the type of task they are designed to support. These tasks may be \emph{prognostic}, i.e. aiming to predict the likely course of a disease, \emph{diagnostic}, i.e. aiming to establish the true nature of a disease, or \emph{theranostic}, i.e. aiming to guide decision-making around allocation of therapies. Increasingly more complex biomarkers are being developed, including for example those depending on the mutation status of multiple genes. While complex biomarkers may increase the power of genomic medicine to support clinical decision-making, their development raises issues in reproducibility, dimensionality, cost efficiency, and interpretability.

\subsubsection{Cancer: a disease of the genome}
Cancer does not require much introduction even to the lay reader; direct or indirect experience of cancer is universal. It is consistently ranked amongst the leading causes of global mortality, and multiple subtypes of cancer are projected to increase in their share of worldwide premature deaths over the coming decades \citep{mathers_projections_2006}, including in the developing world \citep{kanavos_rising_2006}. Beyond its direct death toll, cancer is responsible for the expenditure of trillions of dollars per year in cost of care and lost economic output \citep{wild_world_2020}. While huge gains in the understanding, prevention, and treatment of cancer have been made in recent years, many challenges remains in scalably advancing each of these areas. Modern cancer treatments in particular are often extremely expensive, with drug development costs increasing and consequently inflating the price of access to therapeutics \citep{howard_pricing_2015}. In short, there is much to be hopeful about in oncology, but it is by no means guaranteed that the current revolutions being enjoyed in scientific understanding will translate fully to equitable clinical benefit.

We now turn to the aspects of genomics specifically relevant to cancer. A key starting point is that cancer is not a unitary disease;  two patients’ tumours may be caused by different processes, occur in different tissues, and have very different molecular and physiological effects \citep{wittekind_tnm_2016}. More so than any other disease, every cancer and every tumour are unique. It is therefore natural to ask what unifying features of all cancers justify their joint classification. The modern answer is distinct from the historical answer. Before the advent of molecular genetics, cancers of disparate tissues of origin were grouped together because of the common observation of malignant growths crossing over physiological boundaries. Towards the end of the 19th century, it was recognised that aberrant patterns of cell reproduction were a common feature of cancers \citep{weinstein_history_2008}. By the early 1900s, with the writings of scientists such as Theodor Boveri \citep[see][for a modern translation]{boveri_concerning_2008}, an answer would be formulated foreshadowing our current understanding, although it wouldn't be until far later that this explanation was fully accepted. Boveri proposed that `chromosomal abnormalities' gave rise to the conversion of normal cells to malignant neoplasms. In modern nomenclature, the chromosomal abnormalities to which he referred would be regarded as (a specific kind of) mutations. It is these that lay the groundwork for uncontrolled cellular reproduction and all the other associated hallmarks of cancer\footnote{While some rare cancers such as \gls{pv} may involve uncontrolled production of cells that do not themselves harbour mutations (in this case, mature red blood cells do not contain \glsxtrshort{dna} at all), this is still the downstream effect of mutations in other cell types. For example, in \gls{pv} this is most commonly a mutation of the {JAK2} gene in hematopoietic stem cells \citep{tefferi_jak2_2007}.}, such as avoiding detection from the body's defenses (immunosuppression/evasion), recruiting a local blood supply (angiogenesis), and invasion of separate tissues (metastasis) \citep{hanahan_hallmarks_2011}. Crucially, mutations convert previously normal or benign cell populations into tumours. The mutations that were observable via optical microscopy to scientists in the first half of the twentieth century were structural mutations involving large-scale chromosomal translocations or deletions. It wasn't until the identification of the structure of \glsxtrshort{dna} and its role as the primary mechanism of inheritance by \citet{watson_molecular_1953} and others, and the subsequent development of molecular genetics, that the discrete nature of biological information was fully appreciated. Later developments in \glsxtrshort{dna} sequencing, beginning with the work of \citet{sanger_dna_1977}, allowed a fuller understanding of mutations as changes to the sequence of nucleotide bases that constitutes \glsxtrshort{dna}. The science of cancer continued to progress by associating \glsxtrshort{dna} mutations (errors of cellular information storage), with their mechanistic and functional consequences, in particular those that led to deregulation of normal cell cycle control.

Now that we are armed with a general characterisation of cancer as the consequences of mutations in \glsxtrshort{dna} leading to abnormal reproduction of cells, we can begin to appreciate the reasons for cancer's diversity. Since almost all cells in the body contain \glsxtrshort{dna} and experience regular reproduction, cancer may occur in a wide range of tissues throughout the body\footnote{Tissues/cell types in which cancer is extremely uncommon tend to be those which experience very little reproduction, and so have little chance to accumulate mutations, e.g. neuronal cells and tissues making up the heart.}. Furthermore, the size of the human genome (defined as the combined total of genetic information contained in \glsxtrshort{dna}, comprising of around 20,000 genes and 3 billion nucleotide base pairs) means that, even with cancer being as common a disease as it is, simple statistical reasoning allows us to say with confidence that it is almost inconceivable that two given tumours would carry exactly the same constituent mutations (even without considering complicating factors such as tumour heterogeneity). This leads us to the modern era of molecular biology. Since the completion of the Human Genome Project, high-throughput sequencing, where large portions of the genome in their entirety are sequenced for a biological sample, has become ubiquitous and highly automated. We now have easy access to the precise locations of all mutations in the tumour genomes of many tens thousands of thousands of samples gathered across hundreds of studies via repositories such as \gls{tcga} \citep{weinstein_cancer_2013}. This gives us an opportunity to investigate a variety of fundamental questions with regards to the progression of cancer. 

\subsubsection{Nature vs nurture: cancer beyond the genome}
In the section above we described how modern sequencing technologies allow us to investigate questions in oncology. It's appropriate also to address the inherent limitations of an approach based on tumour genomes. This mirrors a classic debate of nature versus nurture in developmental biology. In this case, we wish to understand the extent to which the dynamics and trajectory of a tumour are predetermined by the genetic damage it carries. We know that cancers are defined by their mutations, but a growing field of investigation is exploring the role of the environment in which a tumour finds itself in allowing it to flourish. 

Since in the cancer-centric chapters of this thesis we'll be focusing on what tumour genomes can tell us, it's important to highlight the information that can't be accounted for with the somatic mutation datasets we'll be using. Firstly, we'll be concerned with datasets describing the \emph{changes} in \glsxtrshort{dna} sequence between the germline (the \glsxtrshort{dna} sequence a patient inherits) and somatic tumour cells. This means that we typically won't be accounting for germline variation between patients. Many cancers have ancestral components, and molecular differences in the normal functioning of cells may certainly impact the abnormal functioning of cells, such as in cancer. Tumours also do not exist in a vacuum -- increasingly, oncology is beginning to acknowledge the important of the tumour \emph{microenvironment} in cancer progression \citep{whiteside_tumor_2008}. The tumour microenvironment consists of non-cancer cells `recruited' to support a tumour, in particular immune cells involved in producing an inflammatory response.

In order to gain a full picture of cancer, therefore, approaches centred purely on characteristic of tumours (such as those presented in this thesis in Chapters~\ref{chap:tmb_estimation} and~\ref{chap:causal_genomics}) must be augmented by investigations into the body's response to tumours. An analogy to this in a non-cancer context can be found in Chapter~\ref{chap:lamp_modelling}. The background to this chapter is the development of diagnostic biomarkers determining the cause of an inflammatory response (in particular, classifying bacterial versus viral infections). The approach taken, rather than attempting to detect the pathogen in question directly, is to detect the body's \emph{response} to infection through the gene expression profile of immune cells. Chapter~\ref{chap:lamp_modelling} focuses on how to make these measurements sensitive and precise.

\subsection{Modern genomics technologies} \label{sec:sequencing}
\subsubsection{DNA sequencing}
Modern genomic medicine is underpinned by the ability to sequence \glsxtrshort{dna} cheaply and quickly. \glsxtrshort{dna} is organised into chromosomes, along each of which many genes are arranged, with further non-coding regions interspersed in between. As described in Section~\ref{sec:central_dogma}, the fundamental units of \glsxtrshort{dna} are nucleotide bases, of which there are four varieties (labelled $C$, $G$, $T$, and $A$). These are organised in groups of length three called codons, which code for the production amino acids. Codons are arranged in sequences such that their amino acids when joined in a chain form proteins - the products of genes. 

The aim of sequencing is to read, base by base, the information content of \glsxtrshort{dna}. This was originally done by Sanger sequencing, a procedure to infer the base composition of a piece of \glsxtrshort{dna} one base at a time \citep{sanger_dna_1977} via electrophoresis. Short-read high-throughput sequencing automates this process via the following  \citep{bentley_accurate_2008}:

\begin{enumerate}
    \item \glsxtrshort{dna} is isolated from a sample and amplified (replicated many times) to ensure good signal.
    \item Purified \glsxtrshort{dna} is broken into many pieces of manageable length.
    \item These short strands act as templates for polymerisation by fluorescently tagged nucleotide bases.
    \item This fluorescnece is automatically detected by imaging to produce many short `reads' corresponding to strands.
    \item These short sequences are matched to a reference human genome to identify where the \glsxtrshort{dna} in the original sample differed from that reference.
\end{enumerate}

The short-read sequencing paradigm, while ideal for speed and scale, relies on the existence of a complete human `reference genome', onto which individual sequences can be matched. While short-read technologies still dominate sequencing studies, other technologies are now increasing in capability and popularity \citep{jain_oxford_2016}.

\paragraph{Tumour/normal variants}
As described in previous sections, in cancer some subset of cells accumulate mutations. These occur via random misreplication of \glsxtrshort{dna} during cell division or exposure to some external mutagen (e.g. cigarette smoke or \glsxtrshort{uv} light). Tumour cells therefore contain \glsxtrshort{dna} with a different sequence to that of the patients' typical (germline) sequence. To analyse these differences systematically with sequencing, typically two samples are collected: one from a tumour and one from normal tissue. The sequences of each of these samples are compared to produce a list of locations at which mutations have occured: these mutations can have a variety of types (replacements, insertions, etc.) and can have vastly differing functional implications. These are often stored/provided in \gls{vcf} or \gls{maf} files. In our use cases we will simplify feature generation from these datasets by only considering counts of mutations of a given type per gene and sample. Finally, note the similarity in relationship \emph{a)} between tumour genome and germline genome, and \emph{b)} between germline genome and human reference genome. Despite this similarity, the actual computational and algorithmic steps taken to produce germline/reference versus tumour/normal comparisons differ because (at least for short-read applications) a preassembled reference genome is typically available for the former.

\paragraph{Targeted sequencing}
While \gls{wgs} and \gls{wes} have become far cheaper in recent years, it is still often not advantageous, particularly in the presence of cost or time constraints, to sequence an entire exome. In particular, if a (relatively small) set of target regions are known to be of interest, `gene panels' designed only to sequence those regions can be deployed fairly simply. In many cases the cost of sequencing a given region at a given depth scales fairly linearly with the total combined length of the genomic regions being targeted. Many commercial gene panels are available (for example for cancer monitoring), often comprising 100-1000 genes.

% In simplest setting, we could express a tumour's mutational profile as a vector, with each component corresponding to whether the tumour-derived and normal sequences match at that point. How long would this vector be? The human genome contains approximately $3\times 10^9$ base locations. This is the dimensionality (which we'll refer to later on as $p$) of naively presented genomic data. We often like to compare the dimensionality of a dataset with the number of samples (which we'll later call $n$) to which we can expect to have access. In this case, unless we have access to tumour profiling for more than a third of all humans on the planet, we can never hope that these numbers will be comparable. We could make a small gain by listing all codons in the genome, labelling a component as one if the codon has been functionally altered by mutations and zero otherwise. Here though we would still have $p = 10^9$.

% We could simplify our data further. Decades of biological research has focused on cataloguing the locations of genes across the genome. We might consider as covariates each of the (approximately $2\times 10^4$) genes, and represent each sample as a vector where each component refers to \textit{a)} whether or not the gene contained a functional mutation, \textit{b)} how many such mutations were present, or \textit{c)} some other representation of the severity of collective mutations presents in the gene, drawing upon known biology. It's important to appreciate the trade-off we've made here: we've imposed an external notion of structure onto our data and in return have greatly reduced the dimensionality (by five orders of magnitude), but in exchange have lost resolution and thus potential information. This gain/sacrifice will be reflected when we choose to make even further structural assumptions in order to construct sensible models.

% \paragraph{Heterogeneity and depth}
% {\color{red} Remove cancer focus}
% Another important concern when dealing with sequencing data is that cells throughout an organism or (particularly) within a tumour are often to some degree heterogeneous. Different sub-populations of cells may have different mutation profiles, which fit into an evolutionary hierarchy within the tumour's history. The importance of understanding the role of heterogeneity is beginning to be appreciated in a clinical context, and this has implications for the type of data that are used. In the context of the high-throughput sequencing pipeline, the relevant quantity is depth: identifying not just one but a variety of tumour sequences at a genomic locus along with the proportion in which they occur means thinking very hard about how best to express that data.

\subsubsection{Gene expression}
As described above, \glsxtrshort{dna} sequencing allows us to investigate the coding sequence of \glsxtrshort{dna} in normal and dysfunctional cells. The central dogma (described in Section~\ref{sec:central_dogma}), however, tells us that this is only a starting point towards understanding the full picture of cellular behaviour. The revolution in cheap \glsxtrshort{dna} sequencing has, in turn, impacted our ability to quantify \glsxtrshort{rna} at scale. However, in our gene expression-focused application study in Chapter~\ref{chap:lamp_modelling} we don't use sequencing-based approaches. In order to give good context to the reasons for this, here we'll briefly describe \glsxtrshort{rna} sequencing, \gls{pcr} (another common lower-throughput method for \glsxtrshort{rna} detection/quantification), and why neither are satisfactory for the aims of Chapter~\ref{chap:lamp_modelling}. The technology we will end up using, know as \glsxtrshort{lamp}, is most similar to \gls{pcr} and so we'll dedicate a reasonable amount of time to discussing how \gls{pcr} looks in practice to motivate both its similarities and differences with \glsxtrshort{lamp}.

\paragraph{RNA sequencing}
\glsxtrshort{rna}-Seq, short for \glsxtrshort{rna} Sequencing, is now a very common method for high-throughput detection and quantification of \glsxtrshort{rna} \citep{wang_rna-seq_2009}. Simplified, \glsxtrshort{rna}-Seq works by converting \glsxtrshort{rna} to \glsxtrshort{dna} via reverse transcription (a common theme in what's to come), then applying standard high-throughput sequencing methods to the resultant \gls{cdna}. This is a slight oversimplification but contains the pertinent steps. In practice, great care needs to be taken in \glsxtrshort{rna} library preparation to ensure that \glsxtrshort{rna} is isolated from \gls{gdna} and of sufficient quality, entailing several further steps.

While \glsxtrshort{rna}-Seq has been revolutionary for exome-wide gene expression studies, many applications remain where it is unnecessary or insufficient. Commonly this is because \glsxtrshort{rna}-Seq, while incredibly cheap compared to historical levels, is still more expensive than simpler techniques for low-throughput analyses, e.g. detection of a single \glsxtrshort{rna} sequence. For these, it is still common to use older methods such as reverse-transcription \gls{pcr}. 

\paragraph{PCR}
 \Glsfirst{pcr} was developed by Kary Mullis while working at Cetus Corporation in 1983, a feat for which he jointly receieved the Nobel Prize in Chemistry in 1993, and has served as a component of much work manipulating \glsxtrshort{dna} molecules ever since \citep{saiki_enzymatic_1985, mullis_specific_1986}. The aim of \gls{pcr} is, given an input \glsxtrshort{dna} molecule, to \emph{amplify} it \emph{in vitro}. By \emph{amplify}, we mean produce many identical/complementary \glsxtrshort{dna} molecules. By \emph{in vitro}, we mean in an artificial context without access to normal cellular machinery. With the basic mechanism of \gls{pcr}, which we will describe below, it is fairly simple to extend to detection/quantification of \glsxtrshort{rna} with a reverse-transcription preparation step.

\Gls{pcr} works by alternating between two steps (referred to together as a `cycle'): a \emph{denaturing} step and a \emph{synthesis} step. Beginning with a double-stranded \glsxtrshort{dna} sequence, a sample is heated until the \glsxtrshort{dna} denature, i.e. splits into two complementary single strands. In the second step, a polymerase known as `Taq' catalyses the synthesis of a complementary sequence onto each single strand. This is initiated by the presence of a primer, a short oligonucleotide comprising some portion of the desired sequence that hybridises to single stranded \glsxtrshort{dna} and acts as a starting point for polymerisation. 

A significant complication to the implementation of \gls{pcr} reactions is that its two stages must occur at different temperatures. The Taq polymerase has an optimal temperature of around 70-80\nolinebreak\textdegree\nolinebreak C, while denaturation occurs around 94-98\nolinebreak\textdegree\nolinebreak C. This variation in temperature determines the progression of subsequent \gls{pcr} reactions and requires a thermocycler. This restricts \gls{pcr} experiments to be conducted in laboratory-like settings, rather than in field or point-of-care applications.

Up until this point we have used terminology pertaining to \emph{detection} and \emph{quantification} interchangeably. In this context, detection refers to an assay returning a binary outcome indicating whether the molecular sequence in question was present in a sample, whereas quantification (also sometimes referred to as \emph{quantitation}) refers to an assay returning a continuous non-negative value denoting the level of abundance of a given molecule. Using the \gls{pcr} assay for quantitation, referred to as \gls{qpcr}, has grown in popularity since the development of automated fluorescence detection. The outputs of \gls{qpcr} analyses are fluorescence curves produced by the detection of fluorescent markers attached to the nucleotide bases used in synthesis, and can track the amount of polymeristion occurring during any given cycle. These typically look as shown in Figure~\ref{fig:sim_fluor_fig}, consisting of relatively flat phases at the beginning and end of a reaction, with more rapid growth phase in between. Detection is typically performed via establishing a threshold (grey dashed line) above which the target is said to be present. Traditionally quantitation has proceeded in a similar fashion, by relating estimated abundance to the number of cycles before the quantitation threshold is met and disregarding all other information.

\begin{figure}
    \centering
    \includegraphics[width=5in]{figures/chapter1/sim_fluor_fig.png}
    \caption{Example fluorescence curve as might be produced by a PCR reaction. Values on $y$-axis on arbitray scale, with example quantitation level shown with grey dashed line.}
    \label{fig:sim_fluor_fig}
\end{figure}

\Gls{pcr} and \gls{qpcr} require the design/selection of a primer (actually two primers, but one is typically the complement of the other). This primer, a short oligonucleotide matching some portion of the target sequence, must be chosen to balance several ideal properties. It must be suitably specific to the target \glsxtrshort{dna} sequence, while also being of an appropriate length. Designing \gls{pcr} primers is both an art and a science, and analagous to one of the problems investigated in Chapter~\ref{chap:lamp_modelling}, albeit in the context of another assay (known as \glsxtrshort{lamp}) that shares some features with \gls{pcr} but, notably, depends on the design of at least six primers. This introduces a much higher degree of complexity, as does the nature of \glsxtrshort{lamp} reactions, which occur far more in parallel than \gls{pcr} reactions which are neatly divided into cycles. The reason to invest in understanding this more complex system is precisely that \glsxtrshort{lamp} is not designed around cycles, because it happens at constant temperature. This eliminates the need for a thermocycler, enabling \glsxtrshort{lamp} to be deployed cheaply and space-efficiently for point-of-care applications such as the one described in Chapter~\ref{chap:lamp_modelling}.

\section{Statistical learning theory} \label{sec:statistical_learning}
\epigraph{``We need data to construct prediction rules, often a lot of it.'' \\
~\\
``For prediction purposes, [linear models] can sometimes outperform fancier
nonlinear models, especially in situations with small numbers of training
cases, low signal-to-noise ratio or sparse data.''}{Elements of Statistical Learning \\
\citep{hastie_elements_2009}}

Now familiar with the most relevant biological concepts, we turn to the mathematical theory and techniques underlying statistical learning, which has experienced a surge of interest in the last few decades, partially fuelled by explosions of data availability across multiple domains, contemporaneous increases in computational power, and the desire to provide satisfying theoretical frameworks to accompany complementary practical advances in machine learning. While so far there is no theory of statistical learning with the capacity to fully describe, explain, or validate the impressive results demonstrated in (for example) deep learning, the language of statistical learning is invaluable in specifying learning models, providing metrics by which they can be measured, and triaging where they go wrong. It is also the language with which we will be attempting to interrogate issues of inference and prediction in cancer genomics. 

Firstly, we lay out some terminology and notation. We often consider a very generic setup, in which we have paired data
\[\mathcal{D}_n = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}.\] 
We refer to this dataset as our `training' data. We do not here consider settings in which components of inputs $x_i$ or ouputs $y_i$ are missing. We model each of these pairs as being drawn independently from a joint probability distribution $P_{X \times Y}$ describing the likelihood of observing any combination of observation $x$ and label $y$. We write that $y_i \in \mathcal{Y}$ for each $i \in \{1,\dots, n\}$. For now we make no assumptions about the nature of the $\mathcal{Y}$: labels may be continuous values for regression ($\mathcal{Y} \subset \mathbb{R}$), discrete values for classification ($\mathcal{Y} = \{0,1\}$), or more complicated hybrid objects such as is the case in survival analysis (see Section~\ref{sec:survival}). We assume that $x_i \in \mathcal{X} \subset \mathbb{R}^p$ for each $i$, so that our observed values are vectors of length $p$ and each element is a real number (possibly restricted to some subset such as the positive reals - this is what $\mathcal{X}$ specifies). We refer to $p$ as the dimension and $n$ as the sample size of our data. 

We wish to fit some model $\mathcal{M}$ to the data. Formally, this model $\mathcal{M}$ refers to a family of probability distributions in which we assume the true distribution $P_{X \times Y}$ lies, sometimes indexed by a discrete set of parameters (referred to as a \emph{parametric} model). The processing of fitting a model refers to choosing an estimated distribution that matches the true distribution as well as possible. This could be in order to make some \emph{inference} about the true distribution, for example through estimating its parameters, which will hopefully shed light on the effect of each of the covariates contained in an observation $x$. Alternatively, we might be principally interested in predicting future values of $y$ from unlabelled observations as accurately as possible. These two aims are often distinguished by the umbrella terms statistical inference and statistical learning.

\subsection{Bayesian and frequentist approaches to statistics}
In this thesis, we will use two different frameworks for describing and fitting statistical learning models. Each of these inherit from a long tradition of statistical research, and debate around the relative merits of each has been extensive and sometimes acrimonious \citep{bayarri_interplay_2004, coles_bayesians_2006, vallverdu_bayesians_2016}. These two schools of thought are \emph{frequentist} and \emph{Bayesian} statistics, and to give a full summary of the history of each and their interactions is well beyond the scope of this introduction. Therefore, we will restrict ourselves here to a brief description of some motivational and practical differences, enough to justify why one may select one over the other for a particular application. This will become relevant in the chapters that follow, as we adopt a Bayesian perspective in Chapter~\ref{chap:lamp_modelling} and a frequentist perspective in Chapters~\ref{chap:tmb_estimation} and~\ref{chap:causal_genomics}.

\subsubsection{Notions of probability}
The divergence between Bayesian and frequentist statistics is in some senses deeply philosophical, and yet often also purely pragmatic. Few would nowadays hold that any working statistician need embrace only Bayesian or frequentist methods as a matter of principle \citep{gelman_holes_2021, bon_being_2023}. However, it is worth emphasising that differences in these two flavours come down to as fundamental a concept as the definition of probability. While most day-by-day decisions around which statistical framework to use would rarely necessitate considering this, it is useful to think about in order to understand where motivating differences have originated, even as an applied statistician.

In the frequentist framing, probabilities are an intrinsic property of some system. Given an experimental setup (the classic examples being a coin toss or die roll), we are invited to imagine the experiment being repeated over and over again with an identical (or at least indistinguishable) starting point. The probability of an event is defined as the limit, as the number of experiments goes towards infinity, of the the proportion of experiments in which the event occurs. So as an increasing number of, for example, fair coin flips are performed, the number of heads obtained should tend towards one half of the total number of flips. It may seem convoluted to define probability as the limit of an infinite sequence, but it is worth remembering that this is no more complex than the standard definition of a real number\footnote{The reals $\mathbb{R}$ are typically constructed as equivalence classes of Cauchy convergent infinite sequences of rational numbers.}.

In the Bayesian framing, probability is a subjective concept and the relates to a particular state of knowledge around a system or event. Bayesian probability may be formulated as a quantification of uncertainty about an event. So for example, in the Bayesian framework there is no inconsistency whatsoever about the `probability' of an event differing between two observers even the underlying experiment is the same, if the observers are privy to different information. This freedom in fact forms the basis of Bayesian inference, where the probability distribution concerning a given quantity is updated on the basis of new data.

Finally, it is worth pointing out that these two notions of probability are simply different \emph{interpretations} of the same axiomatic framework of probability due to \citet{kolmogoroff_grundbegriffe_1933}, or even merely that axiomatic framework being put to different uses. There is no mathematical difference in the laws of probability between the two schools; only in the machinery that probability theory is used to build. In the next section we will see Bayes' rule, a fact about probability that is equally true in any inferential framework. Bayesian statistics having derived its name from the rule\footnote{Or at least shared the derivation of its name with the rule.} is simply a reflection of the central role it plays in doing Bayesian statistics.

\subsubsection{Uncertainty quantification}
The different notions of probability underlying Bayesian and frequentist statistics give rise to different methods of inference and prediction, allowing for and demanding differing interpretations. Let's return to a prediction task as described in the preceding section, and propose the following linear model for $Y$'s dependence on $X$: 
\[Y \ | \ X=x \sim \mathcal{N}(x^T\beta, \sigma^2). \]
Let us suppose for now that $\sigma$ is somehow known and focus on inference for the parameter $\beta$. In the frequentist framing, we have postulated the existence of a true $\beta$ and it is our objective to estimate it as well as possible. We typically do this via \gls{mle}: from our training data $\mathcal{D}_n$ we calculate the likelihood of observing the values $y_i$ that we have under our model, given the observed values of $x_i$ and in an alternate universe where the true value of the model parameter was given by some $\hat{\beta}$. We then choose the $\hat{\beta}$ such that this likelihood is highest and use it as our estimate of the true parameter. We can show that the estimate $\hat{\beta}$ satisfies
\[\hat{\beta} \in \argmin_{\beta \in \mathbb{R}^p} \{{\frac{1}{n}\sum_{i=1}^n (y_i - x_i^T\beta)^2} \}.\]
We would not expect that the \gls{mle} procedure will give us the correct answer every time. Indeed, we may quantify exactly how far away the estimate will typically be from the true value under repeats of the same experiment. In this sense we are doing uncertainty quantification, but the uncertainty being quantified is due to the random noise inherent in the model, i.e. it is still strictly a frequentist uncertainty.

Bayesian inference takes a different approach. Because in Bayesian analysis we are permitted to define a probability distribution purely expressing our belief about a given quantity, we arrive with a predefined distribution, known as the \emph{prior}, whose density we will refer to as $p_{\beta}(\beta)$. We may also (as in the frequentist case) define the likelihood of observing $Y=y$, given all other quantities according to our model. We write this as $p_{Y|X, \beta}(y; x, \beta)$. At this point we may now apply Bayes' theorem, allowing us to recast one conditional probability in terms of another, to recover:
\[p_{\beta|X,Y}(\beta; x, y) = \frac{p_{Y|X, \beta}(y; x, \beta)p_\beta(\beta)}{p_{Y|X}(y;x)}.\]
Here the denominator $p_{Y|X}(y;x)$ is the likelihood of observing $Y=y$ given $X=x$, marginalised across the prior $p_{\beta}(\beta)$. The converse conditional $p_{\beta|X,Y}(\beta; x, y)$ is referred to as the \emph{posterior} distribution, and may be understood as an updated distribution reflecting our new understanding having observed data relating $X$ and $Y$.  Uncertainty quantification in the Bayesian paradigm is therefore far more direct: our `current' distribution for $\beta$ at each point reflects our state of knowledge about the variable, which is liable to change as and when new data becomes available.

It is worth mentioning the impact that a choice of inferential framework has on subsequent prediction tasks. When predicting outputs for future observations in a frequentist framework, typically the best estimates (e.g. \gls{mle} estimates) of a given parameter are used, with uncertainty bounds on the outcome provided due to stochastic elements of the model (in this case the variance $\sigma^2$). This is fundamentally different however in Bayesian analysis, where uncertainty in the parameters of the model can propagate through and are philosophically indistinguishable from stochasticity in outputs. In this sense, Bayesian prediction models can be thought of as continuous weighted mixtures of fixed-parameter models.

\subsubsection{Model fitting}
In performing Bayesian inference, computation of the numerator of the defining Bayes' equation in the previous section is typically computationally easy for any given $\beta$. However, the total \emph{evidence} $p_{Y|X}(y;x)$ in the denominator can be very hard to compute, especially as the dimensionality of $\beta$ increases. This necessitates the use of approximate schemes for producing empirical samples drawn from the posterior distribution of $\beta$. The most well-known of these is \gls{mcmc} \citep{hastings_monte_1970, gelfand_sampling-based_1990}. To produce empirical samples, the \gls{mcmc} algorithm proceeds from a random starting point, and combines proposals of random jumps with a criterion for accepting or rejecting steps based on the ratio of posterior probabilities between the two points. This ratio is available because it does not rely on the evidence. It is possible to show that this procedure forms a Markov chain with a stationary state given by the true distribution.
Alternates to \gls{mcmc} include \gls{hmc} \citep{neal_mcmc_2011}, which will be used for all inferences in this work. \gls{hmc} explores the state space of $\beta$ in similar fashion but allows for a `momentum' component of the current state, allowing for more efficient overall exploration.

\subsection{High-dimensional statistics} \label{sec:high_dimensional}

Informally, we may think of high-dimensional statistics as being concerned with the realm in which the dimensionality of our input data, $p$, is comparable to or greater than the number of training samples $n$ we have available. In this regime the classical asymptotic theory of statistics, which generally relies on an assumption of fixed dimension and considers limiting behaviour as $n \rightarrow \infty$, may fail to apply. Results such as the law of large numbers and central limit theorem are not applicable\footnote{This is not, of course, to say that they are not \emph{true}; simply that we are working in a regime in which their conclusions are not helpful.}. 

High-dimensional statistics attempts to gauge what we can do in regimes such as these. One common approach is to assume that the data being modelled has some low-dimensional structure. For our purposes, this means that we can embed our input data into a lower dimensional space such that this smaller representation of the random variable $X$ contains all or most relevant information about $Y$. This assumption may be motivated by external knowledge about the system being modelled, or may be purely a practical choice in the hope that improvements in predictive or inferential performance can be seen empirically. In order to leverage structural assumptions, we will need to translate them into some algorithmic choice for fitting models. This can be achieved in a variety of ways, including restrictions on the model family $\mathcal{M}$ and adjustments to a given model fitting method. One particularly common approach to the latter is called \emph{regularisation}, and will form an imporant component of much of the work throughout this thesis.

\subsubsection{Assumptions of structure}

Structural assumptions for statistical learning models aim to directly tackle the problem at the heart of high-dimensional statistics: that if each component of our input variable $X$ can have a full and independent contribution to $Y$, then we will not have enough data to recover the information contained in each variable. This is true even in a relatively simple class of models, e.g. \gls{ols}. We therefore assume that some lower-dimensional representation of $X$ is responsible for determining all or most of $Y$'s dependence on $X$. We can formulate this in greatest generality via the principle of \gls{sdr} \citep{adragni_sufficient_2009}. For this, we say that given random variables $(X,Y)$ specified by a joint distribution $P_{X \times Y}$, there exists a sufficient dimension reduction of size $d^*$ if there exists some function $\phi \colon \mathcal{X} \rightarrow \mathbb{R}^{d^*}$ with $d^* < p$ such that $Y$ is conditionally independent of $X$ given $\phi(X)$, i.e.
\[ Y \perp\!\!\!\perp X \ | \ \phi(X). \]
This is a very general form, and while it includes or motivates many practically implemented strategies, in order to be useful these are typically even more restrictive. For example, one may restrict that the sufficient dimension reduction is a linear map \citep{omidiran_high-dimensional_2010, cannings_random-projection_2017}. One particularly common strengthening of \gls{sdr} is an assumption of \emph{sparsity}. Sparsity conveys that only some small subset of input covariates are important. We may formulate this by insisting that the reduction map $\phi \colon \mathcal{X} \rightarrow \mathbb{R}^{d^*}$ has the form
\[ \phi \big(\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_p\end{pmatrix}\big) = 
\begin{pmatrix} x_{i_1} \\ x_{i_2} \\ \vdots \\ x_{i_{d^*}}    \end{pmatrix},\]
where each of the $i_j$ are distinct and $i_j \in \{ 1, \dots, p\}$ for $j \in \{1, \dots, d^*\}$. Here the subset of covariates that are important are the collection $\{x_{i_1}, x_{i_2}, \dots, x_{i_{d^*}}\}$ of size $d^*$. Note that we typically do not know which set of covariates these are in advance, but even assuming that there exists such a subset can prove a surprisingly useful restriction.

It is worth at this point drawing a distinction between two phenomena in statistics and data science both referred to as `sparsity', both of which are relevant to genomics applications. The first is sparse \textit{data}, in which almost all observed data points have the same value (typically zero). Mutation data displays this trait -- the rate at which mutations occur in the genome varies widely across and within cancer types, but rarely exceeds 100 Mut/Mb, i.e. one mutation per $10^4$ nucleotide base pairs \citep{chalmers_analysis_2017}. Sparse data can be exploited in many ways, including through efficient storage and specially tailored algorithms. However, here we will focus on sparse \textit{models}, for which it is assumed that only a small subset of the input covariates are relevant. 

It is also notable that sparsity has a particularly simple interpretation in the context of linear models (and some generalisations of them). Suppose that we once again consider:
\[Y \ | \ X=x \sim \mathcal{N}(x^T\beta, \sigma^2). \]
This does not specify an entire model for $(X, Y)$ (we are making no claims about the marginal distribution of $X$), but does specify a family of distributions for $Y$ conditional on the value taken by $X$, parameterised by $\beta \in \mathbb{R}^p$. We may then say that this model is $d^*$-sparse iff
\[|\beta|_0 \leq d^*, \ \text{where} \ |\beta|_0 := \sum_{j=1}^p \mathbbm{1}\{\beta_j \neq 0\}.\]
This is especially useful in that we've related the structural modelling assumption we're making to the parameters we aim to fit. This will lead naturally to model fitting approaches described in the next section. Methods that encourage sparsity as described here will naturally perform \emph{variable selection} as part and parcel of the model fitting process.

\subsubsection{From structure to regularisation}

Now that we've shown how we may formulate a low-dimensional structural assumption, we explore how this might be translated into an algorithmic implementation or adjustment for model fitting. Picking up where we left off with sparse linear models (a good example case for exploring more general principles), we describe the \gls{lasso} method \citep{tibshirani_regression_1996}. While we don't deploy this directly in the subsequent chapters, many of the methods we utilise (including the group \gls{lasso} in Chapter~\ref{chap:tmb_estimation} and $L_1$-penalised neural networks in Chapter~\ref{chap:causal_genomics}) are expansions of the concept.

Taking the definition of parametric sparsity given in the previous section, we may see that fitting an \gls{ols} model with an assumption of $d^*$-sparsity equates to finding $\hat{\beta}$ satisfying
\[\hat{\beta} \in \argmin_{\substack{\beta \in \mathbb{R}^p \\ |\beta|_0 \leq d^*}} \{{\frac{1}{n}\sum_{i=1}^n (y_i - x_i^T\beta)^2} \}.\]
There are two further innovations required to arrive at the \gls{lasso} method. The first is to note that solving the optimisation above is equivalent, for some penalty term $\eta$ indirectly determined by $d^*$, to solving the \emph{Lagrangian dual}:
\[\hat{\beta} \in \argmin_{\beta \in \mathbb{R}^p} \{\frac{1}{n}\sum_{i=1}^n (y_i - x_i^T\beta)^2 + \eta |\beta|_0\}.\]
To make this equivalence valid for a small value of $d^*$, the `hyperparameter' $\eta$ should be chosen to be large, and vice versa. This is technically valid but computationally intractable, as the $|\cdot|_0$ norm\footnote{Technically a `psuedonorm', as it does not scale homogeneously, i.e. $|\lambda \beta|_0 \neq \lambda|\beta|_0$ in general for scalar $\lambda$.} is non-convex. Practically, to solve this we would have to perform a separate optimisation for each of 
\[\begin{pmatrix} p \\ d^* \end{pmatrix} = \frac{p(p-1)\dots(p-d^*+1)}{d^*!} \] 
potential covariate subsets. The second \gls{lasso} innovation is therefore to replace the $L_0$ norm $|\beta|_0$ with the $L_1$ norm $|\beta|_1 := \sum_{j=1}^p |\beta_j|$. This makes for a convex optimisation problem which can be solved extremely efficiently. Formally, this is known as the \emph{convex relaxation} of the original optimisation problem \citep[for a more thorough introduction see, for example, ][Chapter 7]{wainwright_high-dimensional_2019} and provides many appealing properties. In particular, $L_1$-penalised regression encourages fitting a parameter $\beta$ with many covariates that are exactly zero (as opposed to, for example, $L_2$-penalised regression, also known as \emph{ridge regression}). Our final \gls{lasso} optimisation is therefore to find $\beta$ satisfying
\[\hat{\beta} \in \argmin_{\beta \in \mathbb{R}^p} \{\frac{1}{n}(y_i -x_i^T\beta)^2 + \eta |\beta|_1 \}.\]
The form of alteration that has been made to the model fitting process here, by adding an additional `penalty' encouraging the resultant model to be fitted in a certain way, is known as \emph{regularisation}. Several examples of regularisation will be demonstrated throughout this thesis. 

We may also choose to incorporate structure into our statistical learning models via the choice of model space that we allow. This approach, in which the overall `shape' of a model is chosen to encourage it towards some structural assumption, is sometimes known as \emph{implicit} regularisation (as opposed to the \emph{explicit} regularisation demonstrated above). This might be, for example, via what is known as representation learning, in which a map is learnt embedding inputs into some space, the outputs of which may be used for prediction. A celebrated example of this is in autoencoder and variational autoencoder neural networks \citep{lecun_phd_1987, kingma_auto-encoding_2013}, in which a low-dimensional representation is fitted using a network with a `bottleneck' consisting of relatively few nodes. In Chapter~\ref{chap:causal_genomics}, we will utilise both representation learning and a tailored regularisation approach for approaches to causal inference.

\subsection{Causal inference} \label{sec:causal_inference}
In the final portion of this introduction to techniques in statistical learning, we discuss some of the foundations of causal inference. Causal inference is far from new, but has generated increased attention in recent years. Of particular interest to us in Chapter~\ref{chap:causal_genomics} will be blending flexible regression methods such as random forests and neural networks with approaches in causal inference to extract meaningful conclusions from messy, complex, and disparate data.

Causal inference concerns understanding the effect of some \emph{treatment} on an outcome, potentially in the presence of spurious associations (known as \emph{confounding}) between treatment and outcome due to their shared relationship with an outside factor. Estimating the impact of a treatment on an outcome is difficult because, by definition, we will never observe the outcome for a particular treated sample if we had not performed the treatment, and vice verse. To mitigate this we use a framework called \emph{potential outcomes} due to Donald B. Rubin \citep{rubin_estimating_1974, rubin_causal_2005}.

\subsubsection{Potential outcomes}
Here we continue to consider covariates $X$ taking values in $\mathcal{X} \in \mathbb{R}^p$ (which we may now also sometimes refer to as \emph{confounders}), and a response variable $Y$ taking values in $\mathcal{Y}$. We also introduce a binary treatment indicator variable $A$ taking values in $\{0,1\}$, such that now we have a jointly distributed triple $(X, A, Y)$. We want to understand the effect of altering the treatment $A$ on the outcome $Y$. A natural quantity to estimate might be
\[\bbE[Y | A = 1] - \bbE[Y | A = 0].\]
However, if both $Y$ and $A$ depend on $X$, we might observe some spurious dependencies. In Rubin's framework we therefore want to define reasonably comparable \emph{potential outcomes} of $Y$ in a hypothetical alternate universe in which we had intervened/not intervened to change $A$. 

Let $Y^{(0)}, Y^{(1)}$ refer to potential outcomes taken by $Y$ under intervention with $A=0,1$
respectively. We may therefore say\footnote{In the alternate notation of do-calculus \citep{pearl_causal_1995, pearl_-calculus_2012}, we would say that $\mathbb{P}(Y^{(a)} = y) := \mathbb{P}(Y = y |  \text{do}(A=a))$.} that $Y=Y^{(A)}$. We make two further assumptions: 
\begin{enumerate}
    \item \emph{Positivity:} the propensity function is positive almost everywhere, i.e. 
    \[\pi(x) := \mathbb{P}(A=1 | X = x) \  \text{is such that} \ \pi(X) \in (0,1) \ \text{almost surely.}\]
    \item \emph{No unmeasured confounders:} treatment is `essentially random' given $X$, i.e.
    \[\{Y^{(0)}, Y^{(1)}\} \perp\!\!\!\perp A | X. \]
\end{enumerate}
These are considered sufficient to estimate the {\gls{ate}}, defined below. Note that in the case of a \glsfirst{rct}, all the above apply except that the propensity function $\pi(x)$ is known (often constant) and unconfounded. In this case we get the two conditions above for free.

\subsubsection{Average treatment effect}
The \glsfirst{ate} is defined via $ATE := \bbE[Y^{(1)} -Y^{(0)}]$. The assumptions described in the previous section are sufficient for the \gls{ate} to be identifiable \citep{stone_assumptions_1993}, since 
\begin{align*}
    ATE & = \bbE[Y^{(1)} - Y^{(0)} ] & \\
    & = \bbE[\bbE[Y^{(1)} - Y^{(0)} | X]] & \text{(by the tower property)} \\
    & = \bbE[\bbE[Y^{(1)} | X]] - \bbE[\bbE[Y^{(0)} | X]] & \\ 
    & = \bbE[\bbE[Y^{(1)} | X, A = 1]] - \bbE[\bbE[Y^{(0)} | X, A = 0]] & \text{(by Assumption~2)} \\
    & = \bbE[\bbE[Y | X, A = 1] - \bbE[Y | X, A = 0]], & \label{eq:ate}
\end{align*}
and $X$, $Y$ and $A$ are all observed quantities. Assumption~1 establishes that we can expect to observe samples with both $A=0$ and $A=1$ across the support of $X$, while Assumption~2 gives interchangeability of potential outcomes and observed outcomes conditioned on treatment and confounders. Methods for estimation of $ATE$ have been proposed for a variety of modelling assumptions and contexts \citep{reiersol_confluence_1945, thistlethwaite_regression-discontinuity_1960, rosenbaum_central_1983, abadie_semiparametric_2005, craig_natural_2017, roth_whats_2023}. One common such method is propensity score matching, which relies on partitioning observations into groups with approximately equal propensity scores. Note that 
\begin{align*}
    \mathbb{P}(A=1 | \pi(X), Y^{(1)}, Y^{(0)}) & = \bbE[A | \pi(X), Y^{(1)}, Y^{(0)}]  \\
    & = \bbE[\bbE[A | X, Y^{(1)}, Y^{(0)}] | \pi(X), Y^{(1)}, Y^{(0)}] \\
    & \hspace{50pt} \text{(by the tower property)}\\
    & = \bbE[\bbE[A|X]|\pi(X), Y^{(1)}, Y^{(0)}] \\ & \hspace{50pt} \text{(by Assumption~2)} \\
    & = \bbE[\pi(X) | \pi(X), Y^{(1)}, Y^{(0)}] 
 \\ & \hspace{50pt} \text{(by definition of propensity)} \\
    & = \pi(X) = \mathbb{P}(A=1|\pi(X)), 
\end{align*}
so that $\{Y^{(0)}, Y^{(1)}\} \perp\!\!\!\perp A | X \Rightarrow \{Y^{(0)}, Y^{(1)}\} \perp\!\!\!\perp A | \pi(X)$ (this is due to \citealt{rosenbaum_central_1983}). We can then say that
\[ATE = \bbE[\bbE[Y|\pi(X), A=1] - \bbE[Y|\pi(X), A=0]].\]
To estimate the \gls{ate} we therefore need only to average over the marginal distribution of $\pi(X)$ rather than the entire marginal distribution of $X$, which can be particularly useful when $X$ is high-dimensional. Methods for propensity score matching often begin by using some regression method to produce an estimate $\pihat$ of the propensity function.

Causal inference can be particularly useful when working with \emph{observational} data, i.e. data for which we did not have direct experimental control over the treatment $A$. Situations where causal inference may be used include measuring the impact of administering a given drug from data in which the drug has been given to some patients but not others. Here we may not fully understand whether patterns exist in who has received the drug (e.g. those who are already very ill), and in estimating the \gls{ate} we aim to remove any bias in differences in outcomes (e.g. survival time) for those who were given or were not given the drug. While it is often useful to gauge the utility of a treatment across an entire population via the ATE, it does not necessarily make any progress towards recommending whether treatment will benefit a given patient. For this, we need heterogeneous, conditional, or individual treatment effects. This will be discussed at length in Chapter~\ref{chap:causal_genomics}.

\section{Genomic medicine questions in the language of statistical learning} \label{sec:questions}
\epigraph{``You check the charts, and start to figure it out.''}{LCD Soundsystem \\`All My Friends' (2007)}


At this point, we have covered most of the introductory molecular biology and statistical learning theory that will be necessary to understand later chapters. Here we aim to bring those two threads together by describing how, for each of a set of given application cases, we select a particular set of tools. The tools will be both biological and statistical, and we will describe our motivation in deploying them to solve or partially solve the problem at hand. Each will provide some motivational background to a particular chapter.

\subsection{Signal and noise: understanding genomics technology}
In our first application case we are confronted with the following setup: a gene expression measurement technology, known as \glsxtrshort{lamp}, has been proposed as the only viable technology for transcript profiling in a given medical test. For any sample profiled by \glsxtrshort{lamp}, we use a separate reaction to measure each transcript of interest. The raw data outputted from the \glsxtrshort{lamp} reaction are noisy amplification curves that look fairly similar to \gls{pcr} curves (see Figures~\ref{fig:sim_fluor_fig} and~\ref{fig:gompertz_fit}). From each curve we want to be able to predict the true gene expression associated with that sample and transcript.

This forms the basis of a prediction problem, with input data given by the amplification curves themselves, and outputs given by the true gene expression values (for a training dataset, we have access to these via a gold standard reference technology). However, a few more factors are at play. Firstly, it is well known that \glsxtrshort{lamp} has higher precision and resolution when profiling some transcripts than others. It will be important in the final implementation of the medical test which transcripts are chosen, and this decision will be made not just on the basis of their biological relevance, but also their amenabiliy to measurement with the \glsxtrshort{lamp} technology. We therefore require a means of quantifying our uncertainty when using amplification curves to infer gene expression. We also have some mechanistic knowledge about typical amplification curves, and can quite easily construct models of the shape of these curves based on \glsxtrshort{rna} abundance as an input. Therefore, a framework for predicting gene expression would ideally be based on the ability to invert such models. 

We wish to perform inference in a manner that emphasises and enables the quantification of uncertainty; we have sufficient domain knowledge to select prior parameter distributions that match our understanding of what amplification curves should look like; and we wish in the end to be able to probabilistically invert a conditional model for amplification curves given true gene expression into a conditional model for the opposite. All the considerations above point to using a Bayesian framework in the analysis of \glsxtrshort{lamp} data. This is exactly what we do in Chapter~\ref{chap:lamp_modelling}, in which we do all of the analysis described, save for producing an inverted `gene expression given amplification curve' model: while this aim was certainly the motivation, the work in this chapter is more exploratory, aiming to identify features of \glsxtrshort{lamp} reactions that contribute to improved or worsened quantitative performance.

\subsection{Gene panel selection: working to constraints}
In Section~\ref{sec:high_dimensional}, we discussed some of the terminology associated with high-dimensional statistics. An apt question is, when working with cancer genomics, in what manner of high-dimensional regime are we operating? This obviously depends on the specifics of the question we are trying to answer, the data we have available, and through what lens we view that data. To elaborate on this final point, we consider somatic mutation sequencing as described in Section~\ref{sec:sequencing} and ask how we may process it to produce `tabular' data, for which we can think about values of $n$ and $p$. Mutation data, typically stored in \gls{vcf} or \gls{maf} files, contains a list of mutations, alongside information about which sample a mutation occurred in, where in the genome it occurred, and what form the mutation took. It is natural to consider the number of individual biological samples as $n$, but what about $p$? We could have one `column' (i.e. a separate covariate of our input) devoted to each base in the genome or exome, recording whether or not a mutation was observed at that point. This data would be ultra-high dimensional -- of the order $p\approx3\times10^9$ for genome-wide sequencing, $p\approx 3\times 10^6$ for exome-wide sequencing -- to the extent that it would be almost impossible to use productively, with typical individual studies having on the order of magnitude of thousands of samples at best (\gls{tcga} contains around $2\times 10^4$ samples in total across 33 cancer types; \citealp{cancer_genome_atlas_network_genomic_2015}). It is therefore often useful to take a `gene-level' view. Here we might group exomic bases into their associated gene regions, and for each covariate column take the count of mutations in that gene. This leaves us with a resultant dimensionality of $p\approx 2\times10^4$, a much more manageable size.  

Despite this reduced dimensionality, there are several reasons we might want to whittle down to an even smaller number as part of a prediction task (i.e. to perform variable selection). One is that the resultant selected genes might in themselves be of interest. This is one means of searching for driver genes, i.e. those that when mutated will elevate risk of the development, progression or adaptation of a tumour \citep{hanahan_hallmarks_2000, hanahan_hallmarks_2011}. Driver genes are thought to be relatively rare, and observant readers will note that this is exactly a sparsity assumption -- a regularisation method such as the \gls{lasso} might be helpful. 

Another justification for selecting some small set of genes/genomic loci for a prediction task is that the cost and time to perform sequencing depends (approximately linearly) on the size of the subsection of the genome to be sequenced, and the depth at which it is sequenced. This means that in many practical or clinical environments, cost is a major factor. If a future biomedical test is to be based on sequencing then it may be essential that a concise targeted gene panel is selected, while maintaining enough accuracy that clinicians feel confident in acting upon its predictions. As mentioned above, sparsity may be well-motivated if we are predicting a phenomenon for which we can assume a small set of driver genes. But what if no such assumption is reasonable? What if we are aiming to predict an outcome that (by definition) is known to depend on all regions of the genome, and our need to select a concise set of representatives is purely practical? Will methods from high-dimensional statistics continue to be optimal? This is exactly the question investigated in Chapter~\ref{chap:tmb_estimation}, where we aim to develop concise gene panels for predicting \glsfirst{tmb} and \glsfirst{tib}, each an exome-wide biomarker.


\subsection{Assessing an intervention: getting the most out of data}
While in this thesis we are not principally interested in analysing or accounting for missing data, a fascinating area of study in its own right, our analysis is still often shaped by what events we are and are not able to observe, and the impact this has on the data we do collect. When attempting to formulate a medical question such as ``will a given patient benefit from treatment from a certain drug'', we may fairly straightforwardly formulate this as a prediction task. From a historic dataset we gather some information about each patient, both clinical and genomic, and let this constitute our input values ($x_i$ for patient $i$). We then observe how each patient responded (via an output $y_i$) and what treatment they were given (via a binary indicator $a_i$), and learn to predict $y_i$ on the basis of $x_i$ and $a_i$. As simple as this appears, however, reality has a way of muddying the waters. When investigating whether patients experience a survival benefit from a treatment, we are faced with two limitations to any observational dataset. Firstly, we will only observe for any patient in our historical dataset one outcome at most, any never what \emph{would} have happened had their treatment decision been reversed. Secondly, when assessing survival outcomes we will be limited by the fact that not every patient will die within the timescale of a given study. This is obviously a very good thing, but means that for a substantial portion of our cohort we will not directly observed the outcome of interest, and this can severely impede our ability to fit models to predict it.

It should be of no surprise that, when considering unobserved alternate outcomes under a different treatment, we turn to causal inference. We will however be required to investigate several extensions to the base case laid out in Section~\ref{sec:causal_inference}, to accommodate the use of survival data and to enable answering a more personalised question than can be expressed by \glsxtrlong{ate}. This intersection of restrictions sits at the heart of an active field of research, and will be the focus of Chapter~\ref{chap:causal_genomics}.  

% \subsection{High-dimensional statistics}
% % In these settings results such as the central limit theorem which rely on divergence of the sample size independent of the dimensionality are often not of much use \citep{wainwright_high-dimensional_2019}. This is often the case in cancer genomics. 

% Informally, we may think of high-dimensional statistics to be concerned with the realm in which the dimensionality of our input data, $p$, is comparable to or greater than the number of training samples $n$ we have available. In this regime the classical asymptotic theory of statistics, which generally relies on an assumption of fixed dimension and considers limiting behaviour as $n \rightarrow \infty$, may fail to apply. Classical results such as the law of large numbers and central limit theorem are not applicable. 

% In many statistical models we have a vector $\mathbf{\beta}$ of parameters with at least the same dimension as our data ($\mathbf{\beta} \in \mathbb{R}^q, \ q \geq p$). In generalised linear models (GLMs) the likelihood of an observation $y$ depends upon the data $x_i$ solely via the inner product $x_i^T\mathbf{\beta}$, so that each component of $\beta$ corresponds to the relative importance of its associated covariate. Classically, we would attempt to to estimate the parameter $\beta$ via our observation through a procedure such as likelihood maximisation. However, it is clear in this context that if $p$ is comparable to or larger than $n$ then we have very little chance of accurately inferring the parameter vector $\beta$. For example, we can't expect to simultaneously learn about the effect of 20 covariates if we only have 10 observations: we say here that the model is unidentifiable. 

% High-dimensional statistics attempts to gauge what we can do in regimes such as these. One is approach is to assume the data has some low-dimensional structure. This means that we can embed our data in a lower dimensional space such that the smaller representation of our data contains all or most of the necessary information about the joint distribution $P_{X\times Y}$. We will discuss some common structural assumptions. The simplest and most interpretable is sparsity. \\~\\

% \begin{definition}{\textbf{(Sparsity) "Relatively few covariates are important"} \label{def:sparse}} \\
% Given a vector $\beta \in \mathbb{R}^p$ parameterising a model, we say $\mathbf{\beta}$ is $k$-sparse, for $k \leq p$, if at most $k$ elements of $\mathbf{\beta}$ are non-zero, i.e.
% $$|\mathbf{\beta}|_0 := \sum_{j=1}^{p}\mathbbm{1}\{\beta_j \neq 0\} \leq k. $$
% We can say a model $\mathcal{M}$ parameterised by a vector $\beta$ is $k$-sparse if the vector $\beta$ is $k$-sparse.
% \end{definition}

% Sparsity is a useful assumption to make for a variety of reasons. We are reducing the number of parameters that we must estimate- for a $k$-sparse model, we need only estimate $k$ parameters. Before we do so we need to decide which $k$ parameters are allowed to be non-zero, i.e. to which $k$-dimensional subspace (out of $\begin{pmatrix} p \\ k \end{pmatrix}$ choices) our parameter belongs. In practice this is not a huge issue - some powerful theory from the field of convex optimisation allows for efficient training of sparse models (see the LASSO estimator below). Finally, sparse models are interpretable. A small number of covariates selected for importance can be useful in hypothesis refinement.

% \subsubsection{Sparse data vs sparse models}
% It is worth at this point drawing a distinction between two phenomena in statistics and data science both referred to as 'sparsity', both of which are exhibited in cancer genomics. The first is sparse \textit{data}, in which almost all observed data points have the same value (typically zero). Mutation data displays this trait- the rate at which mutations occur in the genome varies widely across and within cancer types, but rarely exceeds 100 Mut/Mb, i.e. one mutation per $10^4$ nucleotide base pairs \citep{chalmers_analysis_2017}. This sparsity is exploited in the way that tumour/normal \glsxtrshort{dna} data is stored, in file formats such as VCF (variant called format) and MAF (mutation annotated format). Many programming languages and data science packages have data structures optimised for sparse data, and it is also often possible to optimise learning and algorithms for sparse data. However, here we will focus on sparse \textit{models}. These are models where it is assumed that only a small subspace of the covariate space is relevant, via assumptions such as the one described above. 

% This notion that there is some sparse representation of data but that it may not translate directly to a subset of our covariates motivates the more general principle of Sufficient Dimension Reduction (SDR). Sparsity restricts our attention to some small subspace of the covariate space $\mathbb{R}^p$. More generally, we may insist on some important smaller subspace, but one that does not depending on a specific representation of our data $x$. The definition of SDR is somewhat more technical, so those without mathematical background may find it easier to skip.\\~\\

% \begin{definition}{\textbf{(Sufficient Dimension Reduction) "Some small representation of our data contains all the important information"}} \\
% Given $(X,Y)$ drawn from probability distribution $P_{X\times Y}$, we say there exists a sufficient dimension reduction of size $d^*$ if there exists some function $S: \mathbb{R}^p \rightarrow \mathbb{R}^{d^*}$  with $d^* < p$ such that $Y$ is conditionally independent of $X$ given $S(X)$, i.e.
% $$ Y \perp\!\!\!\perp X \ | \ S(X) $$
% For an observation $x$, the image $S(X)$ is a $d^*$-dimensional representation of $x$. As a special case we have linear sufficient dimensional reduction if the function $S$ is a linear projection $A^*:\mathbb{R}^p\rightarrow \mathbb{R}^{d^*}$.
% \end{definition}

% Picking apart this definition, conditional independence means that $Y$ only depends on $X$ through some low-dimensional image. Note that, in contrast to sparsity, we have not made reference to a linear model parameter $\beta$. In fact, in the context of a generalized linear model where $Y$ depends on $X$ only through some function of $\beta^TX$, we can simply take $S(X) = A^*X = \beta^T$ and see that $Y$ admits a sufficient dimensionality condition with $d^*$ = 1. SDR, therefore, is a helpful notion in settings in which we need to apply a non-linear model structure. Methods based on finding sufficient dimension reduction projections by searching through spaces of projections \citep{omidiran_high-dimensional_2010} in combination with non-linear base classifiers are beginning to show promise in a variety of domains  including the analysis of high-dimensional medical data \citep{cannings_random-projection_2017}.


% \subsubsection{Techniques in high-dimensional statistics: selection and regularisation}
% It is all very well imposing assumptions of low-dimensional structure onto our data. How can we now exploit this to produce models that reflect the structural assumptions we've made? One answer is regularisation. Regularisation refers to some penalisation process being applied to the parameters of our model. The intuition is that, given some model parameter $\beta$ of size greater than or equal to the dimension $p$ of our data, and thus of comparable magnitude to our number of samples, we have enough degrees of freedom when fitting the model that we can be guaranteed to produce almost perfect training set results without having done anything more than memorise our data. Therefore we must place restrictions on our parameter, and the trick is to do this as part of the model fitting process by combining a regularisation term to the loss function of our learning procedure (ideally in such a way as to preserve what is known as loss convexity, which allows efficient model fitting).

% Regularisation is applied in practice across a whole range of model types, but is easiest to understand in the context of linear regression, so in the discussion that follows we will restrict ourselves to this setting. 

% In linear regression we have a model $\mathcal{M}_\beta$, parameterised by $\beta$, given by 
% \begin{equation}
% \mathcal{M}_\beta \ : \ Y_i = X_i^T\beta + \varepsilon ,
% \end{equation}

% for some noise $\varepsilon$. We're saying that $Y$ can be approximated by a linear combination of the components of $X$, with the relative weightings of each component given by the components of $\beta$. The loss of our model (a measure of how inaccurately it is predicting across all our data) is given by 

% \begin{equation}
% \mathcal{L(M}_\beta) = \frac{1}{n}\sum\limits_{i=1}^{n} (Y_i - X_i^T\beta)^2. 
% \end{equation}

% In general we choose $\beta$ to minimise this loss for an optimal model, but suppose we wish to find an optimal $k$-sparse model, i.e. one for which $\beta$ is $k$-sparse. Rather than minimising over all possible choices of $\beta$, we're minimising the loss over all values of $\beta$ that are also $k$-sparse:

% \begin{equation}
% \min_{\beta \in \mathbb{R}^p, \ |\beta|_0 \leq k} \{ \mathcal{L(M}_\beta) \}.
% \end{equation}

% Here we face a computational difficulty: we have to separately check each subset of covariates of size $k$ and minimise on that set of possible parameters, then compare them all to find the best. What we do to circumvent this is include a penalisation term for $\beta$ which encourages sparsity alongside the loss function in our optimisation. An obvious choice would be the L0 'norm', $|\beta|_0$, which counts non-zero coefficients. In practice this is not computationally feasible (to be technical, the problem is non-convex and so NP-hard), so instead we use the the L1 norm $|\beta|_1$  given by $\sum_{j = 1}^{p} |\beta_j|$. While this does not explicitly encode sparsity, it turns out that in practice it does produce sparse solutions. This process of replacing a non-convex problem with an easier one is in general called convex relaxation. \\ ~ \\

% \begin{technique}{\textbf{(Regularisation for Sparsity: L1/LASSO)}} \label{sec:lasso}
% Given the setup above, L1 regularised estimation (known in the case of linear regression as the LASSO estimator \citep{tibshirani_regression_1996}) selects $\beta$ solving the following optimisation

% $$\min_{\beta \in \mathbb{R}^p} \{\sum\limits_{i = 1}^{n} \mathcal{L(M}_\beta) + \lambda |\beta|_1 \} $$
% Where $\lambda$ is a positive number chosen to specify how strongly we want to encourage sparsity: different values of $\lambda$ will produce different $k$s in the ouput. A particularly attractive feature of the LASSO selector is that it acts simultaneously as a variable selection and model fitting procedure.
% \end{technique}

% To take stock, we've begun with an assumption that some small subset of our covariates are important in predicting the response $Y$. This assumption might have come from necessity due to data availability, from knowledge of the biological system we are modelling, or from both. We will discuss these possibilities in more depth in the next chapter. We've taken a simple model, and altered it to express this structure, and have done so in a way that's computationally feasible. 

% The specific form of the regularisation we employ can have very subtle effects on the traits it encourages in models, which should motivate us to be very careful when translating the biological knowledge we want to express into our learning systems. For example, adding an identical regularisation term but replacing the L1 norm with the L2 norm ($|\beta|_2 = \sqrt{\sum \beta_i^2}$) does not produce sparse models, but rather models that don't contain large coefficients. The corresponding structural assumption for this is slightly more technical (we can assert a multivariate Gaussian prior on the parameter space for $\beta$). This can be applied in a wide variety of high-dimensional situations, often alongside other forms of regularisation, as a combatant to over-fitting (typically via cross-validation). \\ ~ \\


% \begin{technique}{\textbf{(Regularisation for Dimension: L2/Ridge Regression)}}
% L2 regularised estimation (known as ridge regression in the linear setting \citep{hoerl_ridge_2000}) selects $\beta$ solving the following optimisation
% $$\min_{\beta \in \mathbb{R}^p} \{\sum\limits_{i = 1}^{n} \mathcal{L(M}_\beta) + \lambda |\beta|_2 \} $$
% where again $\lambda$ is a positive value that can be selected by cross-validation to reduce overfitting.
% \end{technique}

% Figure \ref{fig:workflow} describes the workflow of modelling high-dimensional data. The data dimensionality, as discussed in the previous chapter, is the underlying problem, which we address with structural assumptions informed from a mixture of external knowledge and practicality, which are then transformed into a feasible computational problem. Intuition around the biological and also statistical context are applied at each step.

% \begin{figure}[htbp]
% \centering
% \begin{tikzcd}[row sep = small, column sep = large]
% Dimensionality \arrow{r}{Sparsity} \arrow[bend right]{rr}{L_2 \ Penalty} & Structure \arrow{r}{L_1 \ Penalty} & Regularisation \\
% & & \\
% & & \\
% & & \\
% (Data) & (Model) & (Computation) 
% \end{tikzcd}
% \caption{An example of a high-dimensional workflow, where high dimensionality is addressed via the imposition of model structure, in this case sparsity. This is translated into a computationally tractable extension of standard regression model fitting via an L1 penalty. Dimension-induced overfitting is simultaneously managed via L2 regularisation. If sparsity is a reasonable structural assumption, i.e. few covariates have genuine impact, L2 regularisation should have a relatively small impact.\label{fig:workflow}}
% \end{figure}

% For those unsatisfied with the abstract nature of the discussion above, we now attempt to provide more concrete examples.

% {\color{red} OLD SECTIONS


% \section{Genomic medicine questions in the language of statistical learning} \label{sec:questions}
% \epigraph{``You check the charts, and start to figure it out.''}{LCD Soundsystem \\`All My Friends' (2007)}
% \subsection{Biomarker identification}
% We've discussed some of the terminology associated with high-dimensional statistics. We can now express some cancer genomics questions in the same language. We have data with a very high dimensionality $p$: bases, codons or genes ($p \approx 3\times 10^9$, $1 \times 10^9$ and $2 \times 10^4$ respectively) and we would like to predict some outcome, be it a survival value, biomarker signature or other phenotype. Due to the resources and time required to perform whole genome or exome sequencing we often face restrictions in the number of samples at our disposal. The popular Cancer Genome Atlas resource \citep{weinstein_cancer_2013}, for example, contains sequencing data for around 20,000 tumour/normal matched samples. Even if all of these samples were relevant to our study, and we were trying to predict some phenotype $Y$ using gene-level data, we would be working in the $p \approx n$ regime. If we were using codon or nucleotide level information, we would be well into the $p >> n$ regime. In the following we will assume we are working with some gene-level covariates, and investigate what sort of structural assumptions we may wish to make in order to fit tractable and robust models. 

% \subsection{Gene panel design}
% \subsubsection{Sparsity by assumption: driver genes}
% Driver genes in the simplest sense are genes that, when mutated, will elevate risk of the development, progression or adaptation of a tumour \citep{hanahan_hallmarks_2000, hanahan_hallmarks_2011}. They may be grouped roughly into oncogenes and tumour suppressors: oncogenes admit mutations giving some selective advantage to a cancer cell, while tumour suppressors in their standard form protect against aberrant cell growth or apoptosis evasion. Identifying driver genes (or driver sites within genes) amongst the extensive backdrop mutation in tumours is notoriously difficult. Selection pressures produce subtle and often non-obvious patterns of mutation density between neutral and non-neutral genes as well as distinct signatures for oncogenes and tumour suppressors \citep{brown_finding_2019}. Neglecting these difficulties for now, suppose we wish to infer some phenotype $Y$ (again for simplicity we assume that this is continuous and single-valued). We don't have nearly enough data to fully explore the dependence of $Y$ on all genes simultaneously- we have to assume that there are \textit{relatively few relevant features/driver genes}. This is exactly a sparsity assumption - a regularisation method such as LASSO might be helpful. The advantages of this are twofold. We have identified a set of genes of interest, which might form the basis for some targeted prognostic panel, while simultaneously inferring a predictive structure on top of this list of genes. The added interpretability of our model given by assuming a structural restraint is useful when verifying our results in the lab. We've produced a manageable set of interesting genes that can be investigated on a more detailed individual basis.

% \subsubsection{Sparsity by necessity: gene panels for genome-wide biomarkers}
% Another justification for selecting some small set of genes/genomic loci to include in an investigative panel is that the cost and time to perform sequencing depends (approximately linearly) on the size of the subsection of the genome to be sequenced, and the depth at which it is sequenced. This means that in many practical or clinical environments, cost is a major factor. While the cost of whole genome sequencing has decreased at an impressive rate, it is far from being standard of care for cancer patients. It is therefore important that gene-panel style biomarkers are as small as possible, while maintaining enough accuracy that clinicians feel confident in acting upon predictions. This is a particular issue for genome-wide biomarkers, which have gained popularity in recent years, for example in cancer immunotherapy. Examples include tumor mutation burden  \citep{cao_high_2019, samstein_tumor_2019, zhu_association_2019} and indel burden \citep{turajlic_insertion-and-deletion-derived_2017, wu_tumor_2019}, which report density of somatic mutation across the entire cancer genome. In this case all regions of the genome are relevant to greater or lesser extent (Figure \ref{fig:lungmanhat})- the optimal panel for prediction would be the entire genome (or exome, depending on the specific biomarker). However, certain genes may be particularly relevant, for example by taking an active role in \glsxtrshort{dna} repair mechanisms. When estimating such biomarkers, we therefore want to offset the positive predictive contributions of individual genes/loci against the added cost burden given by inclusion in the panel. Analyses of the impact of panel size on predictive power in theoretical and practical settings are becoming more common \citep{budczies_optimizing_2019}.

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=5in]{figures/chapter1/lungmanhat.png}
% \caption{For an additive, genome-wide biomarker such as TMB (Tumour Mutational Burden), all genomic loci are significantly correlated with TMB (unlike in typical GWAS studies). How do we choose a subset that isn't prohibitively large but can reliably estimate the marker via some predictive model? \label{fig:lungmanhat}}
% \end{figure}

% Suppose we have some set $G$ of genes, where $g$ refers to an individual gene with coding sequence of length $n_g$. Now let $P \subset G$ refer to a gene panel comprising a set of genes, and $\mathcal{M}_P$ be a model trained on some data with covariates included according to the gene panel $P$. Then we might wish to solve the optimisation problem 
% \begin{equation}
% \min_{P \subset G} \{\mathcal{L}(\mathcal{M}_P) \} \ \text{such that} \ |P| \leq L,
% \end{equation} 
% where $\mathcal{L}(\mathcal{M})$ is the loss of the model $\mathcal{M}$, $|P| = \sum\limits_{g \in P}n_g$ is the total length of the gene panel $P$ and $L$ is some prescribed maximum panel length. Note the similarity with the LASSO setup described in Section~\ref{sec:lasso}. In the case of a linear model we can similarly reformulate the problem in terms of the parameter $\mathbf{\beta}$, and solve the analogous problem. \\ ~ \\

% \begin{technique}{\textbf{(Weighted L1 Regularisation/LASSO)}} 
% Here we select $\beta$ satisfying the optimisation problem
% $$\min_{\beta \in \mathbb{R}^{|G|}} \{\mathcal{L(M}_\mathbf{\beta}) + \lambda\sum\limits_{g \in G}n_g|\beta_g| \} $$
% where we have again swapped the panel length bound $L$ for the regularisation parameter $\lambda$. Since all the $n_g$ values are positive, this is still a convex optimisation problem and thus can be solved efficiency as in the standard case. Choice of $\lambda$ is less likely to be chosen via cross-fitting, as smaller values of $\lambda$ will always improve predictive power. Instead $\lambda$ will be chosen to control the size of the resulting gene panel.

% \end{technique}
% }

% \section{Modern techniques in high-dimensional statistics and dimensionality reduction}
% \epigraph{``c. 1950: Modern mathematics starts to take off. After that it gets complicated.''}{Professor Stewart's Hoard of Mathematical Treasures (\citeyear{stewart_professor_2010})}

% We conclude with some examples from recent literature of techniques related to dimensionality reduction in modelling genomic data. The examples have been chosen to  demonstrate the structure/regularisation workflow discussed in this chapter, and are small a set of examples rather than (anywhere near) an exhaustive list.

% \subsection{Regularised graphical models}
% In the regression examples discussed previously, the parameters of interest have represented the weighted effect of observed covariates on a label. In supervised and unsupervised cases, we are also often interesting in looking at how closely related different covariates are, through estimating the correlation matrix of the observation variable $X$. If we have an observation of dimensionality $p$, then the covariance matrix will be of size $p^2$, so problems of estimation from small $n$ are even more confounded! 

% Two forms of regularisation are popular, often used in tandem. The first is a sparsity penalty applied to all matrix entries \citep{witten_covariance-regularized_2009}. What does this correspond to structurally? It means that that most pairs of covariates are independent (or at least uncorrelated). This is a very relevant notion in network analysis, where variables are thought to affect each other in a way that can be described by some graphical structure. Sparsity of matrix elements then corresponds to sparsity of the graph describing the network. It is also not uncommon to sparsely penalise precision, defined by the components of the inverse covariance matrix \citep{xia_positive-definite_2017}. 

% Alternately (or in addition), we may wish to limit the number of distinct \textit{patterns} of correlation, so that all covariates display a correlation profile that is made up of a combination of a relatively small set of base signatures. This structure may be fitted for by imposing rank-based regularisation \citep{ye_low-rank_2013, hu_low_2021}. For those wanting a greater appreciation of the theory, the way this is imposed is another good example of convex penalty relaxation (as was achieved by switching from the $L_0$ to $L_1$ norm in sparsity regularisation), where here the nuclear norm is used as the convex relaxation of matrix rank.  


% \subsection{Localised sparsity assumptions}
% We've made an extensive discussion of sparse models in this chapter. We might wonder if there are any generalisations to the assumption that relatively few of our covariates are important throughout all of our samples. One such generalisation would be that for some subsets of our samples sparsity assumptions hold, but that the important covariates may differ from subset to subset within our data. In a localised sparsity setting, we are often given some knowledge of the organisational structure of data, either in a discrete way through a prior partition of the samples or network structure, or in a continuous way through a measure of distance between samples (which may come directly from the input data). We can then fit linear models which are regularised towards sparsity, but where variable selection is allowed to vary between samples, and allowed to vary more between samples that are more distant. This has been applied to the prediction of drug toxicity based on differential gene expression data \citep{yamada_localized_2017}. 

% \subsection{Variational autoencoders}
% For our final example we consider a notion of dimensionality reduction which is more general and which has been studied extensively in the machine learning literature. This nicely elucidates the grey border between statistical and machine learning, and the difficulties and opportunities available to biological research by embracing the latter. 

% Variational autoencoders (VAEs) are a class of neural networks with a variety of architectures and sizes, but whose premise centres around producing an encoding/decoding framework between high dimensional data and a lower dimensional representation \citep{kingma_auto-encoding_2022}. VAEs have an 'hourglass' shape: input data is fed into the network, and information is propagated through layers of progressively smaller size until a bottleneck is reached. The central layer will have some small number of latent nodes. Subsequent layers increase in size, reaching an output of dimension matching the input. VAEs are trained to reproduce the inputs with which they are trained as accurately as possible. We can then view the central latent nodes as an encoding of our input data \citep{zheng_understanding_2019}. This might \textit{a}) contain some insightful information, and \textit{b}) be useful as lower dimensional input data for training other models. 

% In the context of cancer genomics \citep{way_extracting_2018}, VAEs pose two challenges, illustrative of those that machine learning procedures in general must overcome to be useful in a basic research or clinical setting. Firstly, they are highly parameterised compared to the types of model discussed so far. We've discussed at length the balance between data availability and model size, and the significant extra effort necessary to extract information when information is scarce. One of the advantages of deep learning procedures is their versatility and lack of dependence on prior knowledge and assumptions of structure. The cost is that they are very data intensive, prohibitively so in some cases. Secondly, while a VAE's latent nodes may be informative within a network, there is no necessary guarantee that they will be interpretable by a human, nor that biologically relevant features will have been neatly allocated to a single node. Strategies to 'untangle' VAEs are necessary to make biologically relevant predictions \citep{kompa_learning_2020}. 


\section{Summary and thesis outline}
\epigraph{``Now this is not the end. It is not even the beginning of the end. But it is, perhaps, the end of the beginning.''}{WC (1942)}

If this introduction has succeeded in its goals, it should have motivated why techniques from statistical and machine learning have value to add addressing genomics questions in medical applications. In particular, it should have provided the necessary biological and statistical background to understand each of the subsequent chapters. It should also have provided a `high-level' view of why each technique we've selected from the statistical learning arsenal is well-matched to the application case in which we apply it. This motivation will be to some degree restated with each case study, but with more of a focus on providing specific technical context to the decisions made in each investigation.

A recurring theme in each chapter is that the complex structure of data presented in biological research can be a sticking point that, at its full potency, could severely undermine our capacity to answer the question we'd like, and to build the tools that we'd like. The restrictions we face when dealing with biological data can originate from the complexity of the underlying biological systems under investigation, or from external `real-world' factors such as bounds on the cost and time available in order to make a solution fit for purpose.

Even at the current pace of increase of the availability of biological data (in particular from high-throughput sequencing studies), it remains to be seen whether the powerful and general machine learning techniques that have revolutionised other traditional learning problems will be at our disposal in a meaningful way. It is still firmly the case that specific biological and contextual expertise is necessary to optimally leverage the new wealth of data that is available to us. Therefore, to unlock the potential of that data, we need researchers who are able to speak the language of both statistical and biological `camps'. It is not sufficient that researchers in cancer genomics provide data and questions to researchers in statistics/machine learning, nor that statistical researchers push forward with the development of methods without influence from the context of the problems they are attempting to address. Instead, methods need to be crafted bespokely by those who understand what features of biological data are relevant, how those features manifest themselves, and how to exploit them in a mathematically robust way. 

As motivation for the challenges discussed above, it should go without saying that medical genomics in the machine learning age has potential to do a great deal of good in the long term. Yet uncovering a deeper understanding of how diseases work isn't the only worthwhile goal. Designing procedures that can work \textit{now} to be more effective, sometimes crossing a threshold between non-pracitcality and practicality (embedded in some particular context), can have a more immediate benefit. In the clinic, the time scale and cost of data collection are not abstract mathematical problems, so designing a predictive model that maximally leverages the data available (while acknowledging that there is never a guarantee that any particular dataset contains the information necessary to answer a particular question) can be just as enabling as uncovering a new paradigm of disease progression. 

To wrap up this introduction, we'll now briefly (and with some repetition from what's come before) outline the studies discussed in each subsequent chapter. In Chapter~\ref{chap:lamp_modelling} we present new analysis of the \glsxtrlong{lamp} (\glsxtrshort{lamp}) assay on clinical and synthetic samples, motivated by the unmet need for statistcally principled methods for guided \glsxtrshort{lamp} optimisation. To do this, we'll use Bayesian methods for prediction, where the target of prediction is not the medical outcome in question but the technical accuracy of an assay based on a given biological target. 
We continue in Chapter~\ref{chap:tmb_estimation} with another study into the optimal, data-driven design of a biomedical test, albeit in the cancer setting. Here, we are concerned with the prediction of \glsxtrlong{tmb} (\glsxtrshort{tmb}), a key clinical biomarker determing how likely cancer patients are to respond to immunotherapy. Our task is to select a small number of gene targets to form a targeted \glsxtrshort{dna} sequencing panel from which to estimate \glsxtrshort{tmb}. We'll do this with an extension of \glsxtrshort{lasso}-based regularisation. Finally, in Chapter~\ref{chap:causal_genomics}, we extend the previous chapter's work, and investigate the extent to which panel-based genomic markers can be tailored to identify heterogeneous causal effects in immunotherapy response. 

\dobib % renders bibliography (only when compiling for chapter)


\end{document}