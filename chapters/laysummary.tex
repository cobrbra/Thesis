\documentclass[../thesis.tex]{subfiles}

\begin{document}

Of the many diseases that have profound impacts on our society every day, almost all are influenced by factors related to our genes. This can be through the genes we inherit from our parents (our \emph{genome}), the changes that happen to our genome through the course of our life, or the way our genes are used throughout our body. In this century, we've seen incredible advancements in our ability to describe and measure our genomes. Despite this, we feel like we know very little about how our genomic data relates to our experience of disease compared to how much there is to find out. Even when we do have this knowledge, it can be really hard to turn this into technologies and practices that are useful for preventing and treating disease. Why is that?

There are a few reasons. One of the biggest is that our genome is massive. It consists of~20,000 genes, each comprising a sequence of molecular `letters', dispersed throughout an overall genetic sequence around two billion characters long. This has gotten a lot easier, and crucially a lot cheaper -- think \$100,000,000 in the year 2000, less than \$1,000 now -- to read in its entirety for any given person. This practical advantage, however, masks a scientific nightmare. Human DNA contains more letters than there are people who've ever had their genome sequenced. With that many letters, think how many combinations of letters -- how many unique possible humans -- there are, and the complexity of trying to predict anything about a human from their genome alone starts to become obvious. In statistics, we call data like this \emph{high-dimensional}, as a fancy way of saying ``there's a lot of it, in fact more bits of information for each person we look at than there are people available to study''. 

The `curse of dimensionality' described above has arisen from a practical advantage creating a theoretical challenge, but other challenges are themselves practical. While genetic sequencing is now unbelievably cheap compared to two decades ago, it is not free, and for many applications sequencing the entire genome is not necessary or efficient. Additionally, clinical decision-making often has a strong time dependency: a cheap test that takes two weeks to run might just not cut it. We therefore need to think not just about how much information genomic data \emph{can} give us in medicine, but how best to deploy the technologies that we have to make practical impact now.

A final difficulty in the translation of genomic data into medical benefit is both theoretical and practical, and concerns how data is generated and how we combine data from disparate sources. When asking a question like ``can someone's genome help up decide whether to treat them with drug $X$?'', we'll often need to compare genomic data from patients who were treated with drug $X$ with data from patients who weren't. These may have come from different studies, conducted by different organisations, and targeting different populations. To start to untangle all of this, we need a theoretical language to describe the interaction of all of these effects -- the language of \emph{causal inference}. In order to develop tools that are actually useful for clinicians, we'll need a lot of practical insight into the situations in which clinical and treatment decisions are being made.

Having discussed a few of the theoretical and practical problems we'll be trying to address, we'll now describe the specific use cases. In our first application study (Chapter~\ref{chap:lamp_modelling}), we look at trying to understand how to measure gene \emph{expression} (how genes are used in the body) accurately, with a technology that's more difficult to get right than most but that has the highest chance of actually being employed in clinical settings. This technology, LAMP, has its advantage over other (less fiddly) gene expression measurement technologies in that it doesn't require large, expensive and slow bits of machinery, and can give results within 30 minutes. This is crucial for our application space: we're attempting to use this technology to predict whether a patient in emergency care is suffering from a bacterial or viral infection. Making this distinction is crucial in determining whether to treat with antibiotics -- every hour of delay in deciding to give antibiotics to a patient suffering bacterial-driven sepsis can lead to an 8\% increase in risk of death from the infection.

In the second half of this thesis, we're concerned with cancer. Cancer is a natural fit for techniques in genome medicine as \emph{cancer itself is a disease of the genome}. This gives rise in part to cancer's extraordinary diversity: more so than any other disease, every cancer is unique. Indeed, two patients’ tumours may be caused by different things, occur in different places, and have very different effects. In Chapters~\ref{chap:tmb_estimation}~and~\ref{chap:causal_genomics} we address one of the practical challenges described above in the context of cancer. Namely, if a tumour is (to some extent at least) described by the portfolio of mutations in DNA sequence that it has accumulated, do we really need to know every single change in that sequence in order to sensibly guide therapeutic choices? We take two approaches. Firstly, we try and predict a tumour feature known to be associated with improved response to immunotherapy (a type of cancer drug) using as little genomic information as possible. Remember, using little genomic information hopefully means cheaper! Next, we start with set collection of genes and attempt to predict the impact of treating a given patient with immunotherapy purely on the basis of the genetic information contained in those genes.

This entire work is bound together by a few principles:
\begin{enumerate}
    \item Genomic data contains information that is relevant for treating disease;
    \item Modern techniques in data science have the capacity to unlock that data;
    \item These techniques have to be applied in a way that leads to \emph{practical benefit}.
\end{enumerate}
I hope that you enjoy it!

% It’s simple: cancer is not one disease. Two patients’ tumours may be caused by different things, occur in different places, and have very different effects. More so than any other disease, every cancer is unique. To understand this variety, we have to look at the one thing all cancers have in common: mutations. Mutations are where DNA, the information-storing molecule in cells, has been changed by accident somewhere along its sequence. In tumours this causes cells to become detached from the normal rules that govern when cells reproduce and die. Some tumours have lots of mutations throughout their genome (complete DNA sequence), while some have very few. No two tumours share the same pattern of mutations; the human genome is so long that the chances of damage occurring in exactly the same places are minute.

% In the situation we've described there are far more possible outcomes (patterns of mutation) than we will ever have samples to work with. This makes doing classical statistics, which often works on the assumption that we have a large sample size compared to the amount of data contained in each sample, very hard. Research addressing this difficulty is called high-dimensional statistics, and is the mathematical side of what I do. 

% In statistics there are broadly two types of questions we might like to answer. Firstly, given some data, how is that data structured and what correlations exist within it? Secondly, how does our data relate to another, separate, piece of information? We can illustrate these two types of question quite nicely in the context of cancer genomics.

% As we know, tumour cells accumulate mutations. Some are important for the development of the tumour, and some are just along for the ride, caused by the same processes as the important mutations but of little effect. The difficulty is distinguishing the important from the unimportant, when the same set of mutations rarely occurs in two different tumours. To do this we need to build statistical models describing the process by which mutations accumulate, and build into these models structure that reflects our biological knowledge. Relevant knowledge might include how DNA is organised into genes, chromosomes and coding units.  We hope to recover information about what locations in the genome may be important for the success or failure of a tumour, or for causing other mutations. This helps biologists refine their experiments to understand exactly what is happening – no matter how clever our method, they get the final say.

% Next we need to understand how mutations interact with the busy world of a tumour. There is a decades-old debate in biology about how DNA’s role is best interpreted. Some people think of it as an ingredients list for consulting whenever a specific item is needed, others as more like the recipe itself, a set of instructions for running a cell. In cancer, we might ask to what extent the properties of a tumour can be predicted just from its mutations. There is a practical motivation for this, namely that sometimes that’s all the information we have. An emerging technology for sampling tumour DNA is liquid biopsy, where DNA is extracted from blood samples. This is in contrast to solid biopsy, where a tumour is surgical removed. Solid biopsy gives us access to more information, but is very invasive and sometimes impossible. In my (biological) work I try to understand the relationship between mutations and the other processes in the tumour environment. In particular I care about two types of molecules, RNAs and neoantigens, which can be directly measured by solid (but not liquid) biopsy. These molecules are important in determining how well a tumour will respond to a specific set of drugs called immunotherapies. If we can predict how they behave while only being able to see the mutations in a tumour, then we only need to use liquid biopsy when assessing patients for immunotherapy. 

\end{document}