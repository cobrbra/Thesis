%thesis.tex 
%Model LaTeX file for Ph.D. thesis at the 
%School of Mathematics, University of Edinburgh

\documentclass[10pt,twoside,openright]{report} 

\title{Predictors and Consequences of Genomic Instability in Cancer}
\author{Jacob R. Bradley}
\date{2023}

\usepackage[phd]{edmaths}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear, open={(},close={)}}
\usepackage{url}
\usepackage[colorlinks,linkcolor=black,citecolor=blue,urlcolor=blue,breaklinks = true]{hyperref}
\usepackage{multirow}

\usepackage{latexsym,amsmath,amssymb,amsfonts,amsthm,bbm,mathrsfs,breakcites,dsfont,xcolor}
\usepackage{graphicx}
\usepackage{enumerate}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\bbE}{\mathbb{E}}

\usepackage{xcolor}
\usepackage[draft,inline,nomargin,index]{fixme}
\fxsetup{theme=color,mode=multiuser}
\FXRegisterAuthor{tc}{atc}{\color{red} TC}
\FXRegisterAuthor{jb}{abc}{\color{green} JB}

\usepackage[acronym, toc]{glossaries}

\makeglossaries 
\newacronym{tcga}{TCGA}{The Cancer Genome Atlas}
\newacronym{pv}{PV}{Polycythemia Vera}
\newacronym{tmb}{TMB}{Tumour Mutation Burden}
\newacronym{icb}{ICB}{Immune Checkpoint Blockade}
\newacronym{ici}{ICI}{Immune Checkpoint Inhibitor}
\newacronym{msi}{MSI}{Micro-Satellite Instability}
\newacronym{lasso}{LASSO}{Least Absolute Shrinkage and Selection Operator}
\newacronym{tib}{TIB}{Tumour Indel Burden}
\newacronym{ctla4}{CTLA-4}{Cytotoxic T Lymphocyte Associated protein 4}
\newacronym{pdl1}{PD-L1}{Programmed Death Ligand 1}
\newacronym{wes}{WES}{Whole Exome Sequencing}
\newacronym{ctdna}{ctDNA}{Circulating Tumour DNA}
\newacronym{bmr}{BMR}{Background Mutation Rate}
\newacronym{nsclc}{NSCLC}{Non-Small Cell Lung Cancer}
\newacronym{auprc}{AUPRC}{Area Under Precision-Recall Curve}
\newacronym{ectmb}{ecTMB}{Estimation and Classification of Tumour Mutation Burden}

\renewcommand{\thesection}{\arabic{section}}


\begin{document}


\maketitle

\declaration

\dedication{To Morton...}

\begin{abstract}
In this work, we employ a variety of methods, some novel, from high-dimensional statistics and machine learning to high-throughput cancer genomics data. We restrict our enquiry in the following two ways. Firstly, we are concerned with understanding genomic instability and the hypermutated phenotype, particularly in the context of its relevance to immunotherapy. Secondly, while we use a variety of 'omics data types to inform our understanding, our specific aim will be to make clinical predictions on the basis only of whole-genome, whole-exome, or targeted panel sequencing of the tumour genome. This can be motivated via both via scientific interest and clinical practicality. While somatic mutations have been understood for a long time as the instigators and drivers of tumourigenesis, it is now well known that the tumour environment and external factors play key roles in cancer development and spread. It is therefore of key importance to appreciate the nature of this balance. We formulate our interrogation of this broad question as a signal-to-noise problem, and attempt to ascertain to what extent the downstream properties of tumours can be predicted purely on the basis of the mutations they carry. This is in turn directly applicable to a growing area of clinical practice, liquid biopsy. Unlike solid biopsy, the nature of liquid biopsy means that we are only able to sequence tumour DNA, requiring any decisions based on liquid biopsy to be based purely on somatic mutations, potentially for some subset of the genome. We present a blueprint for the design, development and estimation of clinical biomarkers of response to immunotherapy based on generative models of the genome-wide landscape of molecular processes in tumour cells. From such models we can also make inferences about the underlying mechanisms leading to genomic instability, and how the hypermutated phenotype interacts with the body's natural defences against cancer. 
\end{abstract}


\chapter*{Lay Summary}
\label{chap:laysummary}
\addcontentsline{toc}{chapter}{\nameref{chap:laysummary}}

Cancer is a disease that has profound impacts on our society every day. Despite this, we feel like we know very little about cancer relative to how much there is to know. Why is this? 

It’s simple: cancer is not one disease. Two patients’ tumours may be caused by different things, occur in different places, and have very different effects. More so than any other disease, every cancer is unique. To understand this variety, we have to look at the one thing all cancers have in common: mutations. Mutations are where DNA, the information-storing molecule in cells, has been changed by accident somewhere along its sequence. In tumours this causes cells to become detached from the normal rules that govern when cells reproduce and die. Some tumours have lots of mutations throughout their genome (complete DNA sequence), while some have very few. No two tumours share the same pattern of mutations; the human genome is so long that the chances of damage occurring in exactly the same places are minute.

In the situation we've described there are far more possible outcomes (patterns of mutation) than we will ever have samples to work with. This makes doing classical statistics, which often works on the assumption that we have a large sample size compared to the amount of data contained in each sample, very hard. Research addressing this difficulty is called high-dimensional statistics, and is the mathematical side of what I do. 

In statistics there are broadly two types of questions we might like to answer. Firstly, given some data, how is that data structured and what correlations exist within it? Secondly, how does our data relate to another, separate, piece of information? We can illustrate these two types of question quite nicely in the context of cancer genomics.

As we know, tumour cells accumulate mutations. Some are important for the development of the tumour, and some are just along for the ride, caused by the same processes as the important mutations but of little effect. The difficulty is distinguishing the important from the unimportant, when the same set of mutations rarely occurs in two different tumours. To do this we need to build statistical models describing the process by which mutations accumulate, and build into these models structure that reflects our biological knowledge. Relevant knowledge might include how DNA is organised into genes, chromosomes and coding units.  We hope to recover information about what locations in the genome may be important for the success or failure of a tumour, or for causing other mutations. This helps biologists refine their experiments to understand exactly what is happening – no matter how clever our method, they get the final say.

Next we need to understand how mutations interact with the busy world of a tumour. There is a decades-old debate in biology about how DNA’s role is best interpreted. Some people think of it as an ingredients list for consulting whenever a specific item is needed, others as more like the recipe itself, a set of instructions for running a cell. In cancer, we might ask to what extent the properties of a tumour can be predicted just from its mutations. There is a practical motivation for this, namely that sometimes that’s all the information we have. An emerging technology for sampling tumour DNA is liquid biopsy, where DNA is extracted from blood samples. This is in contrast to solid biopsy, where a tumour is surgical removed. Solid biopsy gives us access to more information, but is very invasive and sometimes impossible. In my (biological) work I try to understand the relationship between mutations and the other processes in the tumour environment. In particular I care about two types of molecules, RNAs and neoantigens, which can be directly measured by solid (but not liquid) biopsy. These molecules are important in determining how well a tumour will respond to a specific set of drugs called immunotherapies. If we can predict how they behave while only being able to see the mutations in a tumour, then we only need to use liquid biopsy when assessing patients for immunotherapy. 

\printglossary[type=\acronymtype]

\tableofcontents

\chapter{Introduction}

\section{Cancer as a disease of the genome}
Cancer does not require much introduction even to the lay reader; direct or indirect experience of cancer is universal. It is consistently ranked amongst the leading causes of global mortality, and multiple subtypes of cancer are projected to increase in their ranking and share of worldwide premature deaths over the coming decades \citep{mathers_projections_2006}, including in the developing world \citep{kanavos_rising_2006}. Beyond its direct death toll, cancer is responsible for the expenditure of trillions of dollars per year in cost of care and lost economic output \citep{wild_world_2020}. While huge gains in the understanding, prevention and treatment of cancer have been made in recent years, many challenges remains in scalably advancing each of these three categories. Modern cancer treatments in particular are often extremely expensive, with drug development costs increasing and consequently inflating the price of access to therapeutics \citep{howard_pricing_2015}. In short, there is much to be hopeful about in oncology, but it is by no means guaranteed that the current revolutions being enjoyed in scientific understanding will translate fully to equitable clinical benefit.

In order to understand the state of play in cancer research, we need to know a little about the nature of cancer itself, and a little about the nature of modern molecular biology. A key starting point is that cancer is not a unitary disease;  two patients’ tumours may be caused by different processes, occur in different tissues, and have very different molecular and physiological effects \citep{wittekind_tnm_2016}. More so than any other disease, every cancer is unique. It is therefore natural to ask what unifying features of all cancers justify their joint classification. The modern answer is distinct from the historical answer. Before the advent of genetics, cancers of disparate tissues of origin were grouped together under the unifying observation of malignant growths crossing over physiological boundaries. Towards the end of the 19th century, it was recognised that aberrant patterns of cell reproduction were a common feature of cancers \citep{weinstein_history_2008}. By the early 1900s, with the writings of scientists such as Theodor Boveri \citep[see][for a modern translation]{boveri_concerning_2008}, an answer would be formulated foreshadowing our current understanding, although it wouldn't be until far later that this explanation was fully accepted. Boveri proposed that 'chromosomal abnormalities' gave rise to the conversion of normal cells to malignant neoplasms. In modern nomenclature, the chromosomal abnormalities to which he referred would be regarded as (a specific kind of) mutations. It is these that lay the groundwork for uncontrolled cellular reproduction and all the other associated hallmarks of cancer\footnote{While some rare cancers such as \acrfull{pv} may involve uncontrolled production of cells that do not themselves harbour mutations (in this case, mature red blood cells do not contain DNA at all), this is still the downstream effect of mutations in other cell types. For example, in \acrshort{pv} this is most commonly a mutation of the \textit{JAK2} gene in hematopoietic stem cells \citep{tefferi_jak2_2007}.}, such as avoiding detection from the body's defenses (immunosuppression/evasion), recruiting a local blood supply (angiogenesis), and invasion of separate tissues (metastasis) \citep{hanahan_hallmarks_2011}. Crucially, mutations convert previously normal or benign cell populations into tumours. The mutations that were observable via optical microscopy to scientists in the first half of the twentieth century were structural mutations involving large-scale chromosomal translocations or deletions. It wasn't until the identification of the structure of DNA and its role as the primary mechanism of inheritance by Franklin, \citet{watson_molecular_1953} and the subsequent development of molecular genetics that the discrete nature of biological information was fully appreciated. Later developments in DNA sequencing, beginning with the work of \citet{sanger_dna_1977} allowed a fuller understanding of mutations as changes to the sequence of nucleotide bases that constitutes DNA. The science of cancer continued to progress by associating DNA mutations (errors of cellular information storage), with their mechanistic and functional consequences, in particular those that led to deregulation of normal cell-cycle control.

Now that we are armed with a general characterisation of cancer as the consequences of mutations in DNA leading to abnormal reproduction of cells, we can begin to appreciate the reasons for cancer's diversity. Since almost all cells in the body contain DNA and experience regular reproduction, cancer may occur in a wide range of tissues throughout the body\footnote{Note that tissues/cell types in which cancer is extremely uncommon tend to be those which experience very little reproduction, and so have little chance to accumulate mutations, e.g. neuronal cells and tissues making up the heart}. Furthermore, the size of the human genome (defined as the combined total of genetic information contained in DNA, comprising of around 20,000 genes and 3 billion nucleotide base pairs) means that, even with cancer being as common a disease as it is, simple statistical reasoning allows us to say with confidence that it is almost inconceivable that two given tumours would carry exactly the same constituent mutations (even without considering complicating factors such as tumour heterogeneity). This leads us to the modern era of molecular biology. Since the completion of the human genome project \citep{lander_initial_2001}, high-throughput sequencing, where large portions of the genome in their entirety are sequenced for a biological sample, has become ubiquitous and highly automated. We now have easy access to the precise locations of all mutations in the tumour genomes of many tens thousands of thousands of samples gathered across hundreds of studies via repositories such as \acrfull{tcga} \citep{weinstein_cancer_2013}. This gives us an opportunity to investigate a variety of fundamental questions with regards to the progression of cancer. One of these mirrors a classic debate of nature versus nurture in developmental biology. In this case, we wish to understand the extent to which the dynamics and trajectory of a tumour are pre-determined by the genetic damage it carries. We know that cancers are defined by their mutations, but a growing field of investigation is exploring the role of the environment in which a tumour finds itself in allowing it to flourish. For the remainder of this section, we will elaborate on the balance between these two views of the tumour genome.
\subsection{Nature: mutations in the driver's seat}
- Discussion of the ways in which mutations drive cancer, in particular oncogenes and tumour suppressors.
- Note that mutations can come from internal or external factors.
\subsection{Nurture: A warm (micro-) environment}
- Pull back on the importance of mutations, and look at the micro-environment, in particular hot and cold microenvironments. \citep{keenan_genomic_2019} \citep{boulter_fibrotic_2020}



\section{Genomic instability}
\subsection{The molecular mechanics of DNA damage}
\subsection{The hypermutated phenotype}
\subsection{Mismatch repair}
\section{Immunotherapy, checkpoint blockade and beyond}


\section{Clinical practice and pharmacoeconomics}
\subsection{Biomarkers as proxies of immunogenicity}
Discuss \acrshort{tmb} and \acrshort{msi}.

\subsection{Liquid biopsy}
\citep{jensen_association_2020} \citep{genovese_clonal_2014} \citep{razavi_high-intensity_2019} \citep{schweizer_clinical_2019} \citep{annala_circulating_2018} \citep{goodall_circulating_2017}
\subsection{Targeted gene panels}

\section{Roadmap}

\chapter{The Generative/Predictive Blueprint for Biomarker Estimation}
This chapter consists, in part, of discussion adapted from 'Dimensionality and Structure in Cancer Genomics: A Statistical Learning Perspective' \citep{bradley_dimensionality_2020} (chapter 3 of 'Artificial Intelligence in Oncology Drug Discovery and Development' \citep{cassidy_artificial_2020}).

\section{High-dimensional, sparse and bursty: generative models of mutation}
\section{Learning from learning: biomarker prediction}
\section{The double role of regularisation}

\chapter{A Simple Example: Tumour Mutation Burden}
\section{Introduction}
It has been understood for a long time that cancer, a disease occurring in many distinct tissues of the body and giving rise to a wide range of presentations, is initiated and driven by the accumulation of mutations in a subset of a person's cells \citep{boveri_concerning_2008}. In the last decade there has been an explosion of interest in cancer therapies targeting immune response, with immune checkpoint blockade therapy winning the Nobel Prize in Physiology or Medicine in 2018 \citep{ledford_cancer_2018}. Immune checkpoint inhibitors/blockades (\acrshort{ici}/\acrshort{icb}s) are antibodies that bind to proteins on the surface of immune cells. These surface proteins are known as immune checkpoints, the two most widely targeted being \acrfull{ctla4} and \acrfull{pdl1} \citep{buchbinder_ctla-4_2016}. Inhibition of immune checkpoints with antibodies promotes a more aggressive immune response to cancer in some patients \citep{pardoll_blockade_2012}.



Exome-wide prognostic biomarkers for immunotherapy are becoming increasingly well-established, with \acrfull{tmb} \citep{zhu_association_2019, cao_high_2019} in particularly being widely employed in clinical practice \citep{stenzinger_tumor_2019}. The \acrshort{tmb} of a tumour is the total number of (non-synonymous) mutations occurring throughout the exome, and can be thought of as a proxy for how easily a tumour cell can be recognised as foreign by immune cells. However, the cost of measuring \acrshort{tmb} using \acrfull{wes} \citep{sboner_real_2011} currently prohibits its widespread use as standard-of-care, particularly given the small proportion of patients for whom \acrshort{icb} is beneficial in many cancer types \citep{nowicki_mechanisms_2018}. The cost of sequencing, both financial and in terms of the time taken for results to be returned, are particularly problematic in situations where high-depth sequencing is required, such as when utilising blood-based \acrfull{ctdna} from liquid biopsy \citep{gandara_blood-based_2018}. These issues are also encountered when measuring more recently proposed biomarkers such as \acrfull{tib} \citep{wu_tumor_2019,turajlic_insertion-and-deletion-derived_2017} which only include mutations of strong immunogenicity. There is, therefore, a demand for cost-effective approaches for estimating these biomarkers \citep{fancello_tumor_2019}.

In this paper we propose a novel, data-driven method for biomarker estimation, based on a generative model of how mutations arise in the tumour exome.  More precisely, we model mutation counts as independent Poisson variables, where the mean number of mutations depends on the gene of origin and variant type, as well as the \acrfull{bmr} of the tumour. Due to the ultrahigh-dimensional nature of sequencing data, we use an $l_1$ regularisation penalty when estimating the model parameters, in order to reflect the fact that in many genes' mutations arise purely according to the \acrshort{bmr}. In addition this identifies a set of genes that are mutated above or below the background rate, which are often of clinical interest. The flexibility of this model facilitates the construction of a new estimator of \acrshort{tmb}, based on a weighted linear combination of the number of mutations in each gene. The vector of weights is chosen to be sparse (i.e. have many entries equal to zero), so that our estimator of \acrshort{tmb} may be calculated using only the mutation counts in a subset of genes. In particular, this allows for accurate estimation of \acrshort{tmb} from a targeted gene panel, where the panel size (and therefore the cost) may be determined by the user.  We demonstrate the excellent practical performance of our framework using non-small cell lung cancer dataset \citep{chalmers_analysis_2017}, including a comparison with the existing state-of-the-art data-driven approaches for estimating \acrshort{tmb}, as well as estimators based on commercially available targeted gene panels.  

Aside from its excellent performance at predicting \acrshort{tmb}, our method has a number of secondary benefits. It can be adapted very easily to predict \acrshort{tib}, due to our separately modelling mutations of different variant type. For estimating \acrshort{tmb} from a targeted gene panel, we are able to either select an optimal set of genes of a given total length, produce predictions from a pre-defined set of observed genes, or augment a pre-defined set of genes by a given amount of genomic space. Inferring background mutation rate by applying regularisation to our generative model rather than by comparing synonymous and non-synonymous mutation rates (as has been proposed elsewhere) allows us to take into account known non-selective effects on mutation rate, including chromatin configuration \citep{makova_effects_2015} and transcription-dependent repair mechanisms \citep{fong_intertwined_2013}. This also allows our method to be applied to datasets for which synonymous mutation counts are not available. The learned parameters of our model are interpretable and correspond to tangible biological effects and processes. We are also able to provide heuristic error bounds.

Due to the emergence of \acrshort{tmb} as a biomarker in recent years, a variety of groups have considered methods for its estimation. A simple and common way to estimate \acrshort{tmb} is via the proportion of mutated codons in a targeted region. \citet{budczies_optimizing_2019} investigate how the accuracy of prediction made in this way are affected by the size of the targeted region, when mutations are assumed to occur at uniform rate throughout the genome. \citet{yao_ectmb_2020} modelled mutations as following a negative binomial distribution while allowing for gene-dependent rates, and estimate \acrshort{tmb} using maximum likelihood. Linear regression models have been used for both panel selection \citep{lyu_mutation_2018} and for biomarker prediction \citep{guo_exon_2020}. A review of some of the issues arising when dealing with targeted panel-based predictions of \acrshort{tmb} biomarkers is given by \citet{wu_designing_2019}. Finally, we are unaware of any research dedicated to methods for efficiently estimating \acrshort{tib}, a more difficult task due to the low relative abundance of indel mutations and known reliability issues for sequencing genomic loci of highly repetitive nucleotide constitution \citep{narzisi_challenge_2015}.

The remainder of the paper is as follows. After a brief discussion of our data sources and terminology in Section \ref{sec:dataterminology} we describe our generative model of mutation in Section \ref{sec:genmodel}, and show how regularised model fitting can allow us to infer a background mutation rate. In Section \ref{sec:linearestimator} we show how to produce a sparse linear estimator via a convex optimisation based on the group \acrshort{lasso}. We present experimental results of our procedures in Section \ref{sec:experimentalresults}, comparing our predictive performance to that of various state of the art statistical learning methods and commercially available gene panels.
%In Section \ref{sec:theoreticalresults} we provide some theoretical discussion of our techniques, illustrated by simulated datasets. 
We conclude in Section \ref{sec:conclusion} with some remarks about our method's utility in the context of a wide span of literature around biomarker estimation, and future avenues for research. 
% At various points it is worth discussing possible biological interpretations that can be drawn from our results. 
% When this is not directly relevant to the question of biomarker estimation we leave these discussions for treatment in Appendices \ref{appendix:indelselection} and \ref{app:goodnessoffit}. Appendix \ref{app:errorbounds} contains formal derivations of expressions for error bounds.



\section{Methodology}
\subsection{Data and terminology \label{sec:dataterminology}}
To demonstrate our proposal we make use of the non-small cell lung cancer dataset produced by \citet{campbell_distinct_2016}, which contains data from 1144 patient-derived tumours. The patients had a variety of prognoses and were mixed in age, sex and smoking history (see Figure~\ref{fig:clinicaldata}). For each sample we have the genomic locations and variant types of all mutations identified, as well as other covariates such as gender, age and smoking history. In Figure \ref{fig:trainingdata}\textbf{A} we see that mutations counts are distributed over a very wide range, as is the case in many cancer types \citep{chalmers_analysis_2017}. For simplicity, we only use nonsynonymous variants, which we group into seven types: the four most common are missense mutations, nonsense mutations, frameshift insertions/deletions and splice site mutations. The final three variant types, which are in-frame insertions/deletions, nonstop mutations and translation start site mutations, are much rarer. We show the respective frequencies of the non-synonymous mutations described in Figure \ref{fig:trainingdata}\textbf{B}. Frameshift insertion/deletion (also known as indel) mutations are of particular interest to us, but contribute only a small proportion ($<4\%$) of mutations, which will have important consequences when trying to predict their abundance. 

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{clinical_data_demographics.png}
\vspace*{-5mm}
\caption{Demongraphic data for the clinical cohort in \citet{campbell_distinct_2016}. \textbf{A}: Violin plots of age for patients, stratefied by sex. \textbf{B}: Stacked bar chart of patients' smoking histories. Shade corresponds to cancer stage diagnosis at time of measurement. \label{fig:clinicaldata}}
\vspace*{-2mm}
\end{figure}

% \begin{table}
% \begin{center}
% \begin{tabular}{ || c | c || c | c ||}
% \hline
% \hline
% \multicolumn{2}{||c||}{Sex} & \multicolumn{2}{c||}{Cancer Stage} \\ 
% \hline
% Female & 468 (40.9 \%)  & I &  \textbf{575} (50.3\%)\\
% Male  & \textbf{673} (58.8\%)  & II & 307 (26.8\%)\\
% \cline{1-2}
% \multicolumn{2}{||c||}{Age} & III  & 192 (16.8 \%)\\
% \cline{1-2}

% $80+$ & 59 (5.16\%)  & IV & 38 (3.32\%)\\
%  \cline{3-4}
% $70-79$& \textbf{340} (29.7\%) & \multicolumn{2}{c||}{Smoking History} \\
% \cline{3-4}
% $60-69$ & 339 (29.6\%) &RS $\leq 15$  & \textbf{407} (35.6\%) \\
% $50-59$ & 170 (14.9\%) & RS $>15$  & 212 (18.5\&) \\
% $40-49$ &44 (3.85\%) & RS (NA) & 86 (7.42\%) \\
% $<40$ & 3 (0.262\%) & CS & 271 (23.7\%) \\
%  & & NS & 111 (9.70\%)\\

% \hline
% \hline
% \end{tabular}
% \end{center}
% \caption{Summary statistics for the \citet{campbell_distinct_2016} dataset. \label{table:summarystatistics}}
% \end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{data_stats.png}
\vspace*{-5mm}
\caption{ Dataset-wide distribution of mutations. \textbf{A}: Violin plot of the distribution of \acrshort{tmb} and \acrshort{tib} across training samples. \textbf{B}: The relative frequency of different mutation types. \label{fig:trainingdata}}
\vspace*{-2mm}
\end{figure}

It is useful at this point to introduce the notation used throughout the paper. We let the set $G$ denote a relevant collection of genes. For our purposes $G$ will be considered to enumerate the entire exome. For a gene $g \in G$, let $\ell_g$ be the length of $g$ in nucleotide bases, defined by the maximum coding sequence\footnote{Maximum coding sequence is defined as the collection of codons that may be translated for some version of a gene, even if all the codons comprising the maximum coding sequence are never simultaneously translated.}. A gene panel $P$ is a subset $P \subseteq G$, and we may write $\ell_P := \sum_{g \in P} \ell_g$ for the total length of a panel $P$. Let $S$ denote the possible variant types in our data (e.g.~in the dataset mentioned above, $S$ contains the seven possible non-synonymous variants). Now, for $i \in \{0, 1, \ldots, n\}$, let $M_{igs}$ denote the count of mutations in gene $g \in G$ of type $s \in S$ in the $i$th sample. Here $n$ refers to the number of samples in the training dataset, and the label $i=0$ will be used to refer to an unseen test sample.  In order to define the exome-wide biomarker of particular interest, we specify a subset of mutation types $\bar{S} \subset S$, and let
\begin{equation}
T_{i\bar{S}} := \sum_{g \in G} \sum_{s \in \bar{S}} M_{igs}.
\label{eq:biomarker}
\end{equation}
% For example, if $\bar{S} = S$, i.e. the set of all non-synonymous mutation types in $S$, then $T_{i\bar{S}}$ is tumour mutation burden. Alternatively, if $\bar{S}$ contains only frameshift insertions and deletions, then $T_{i\bar{S}}$ refers to indel burden. Finally, let 
% \begin{equation}
%   \label{eq:TStest}
% T_{\bar{S}} := \sum_{g \in G}\sum_{s \in \bar{S}} M_{gs}
% \end{equation}
% 
For example, including all non-synonymous mutation types in $\bar{S}$ specifies $T_{i\bar{S}}$ as \acrshort{tmb} for sample $i$, while letting $\bar{S}$ contain only indel mutations gives \acrshort{tib}. Our main goal is to predict $T_{0\bar{S}}$ after only observing $\{M_{0gs}: g \in P, s \in S\}$, where the panel $P \subseteq G$ has length $\ell_P$ satisfying some upper bound. When it is clear from context that we are referring to a test sample and a specific choice of biomarker, we will simply write $T$ for $T_{0\bar{S}}$.


\subsection{Generative model \label{sec:genmodel}}
We now describe the main statistical model that underpins our methodology. To account for selective pressures within the tumour and differences in transcription rate between different genes, we allow the rate at which mutations occur to depend on the gene and type of mutation. We also include a sample-dependent parameter, to account for differing levels of mutagenic exposure between tumours that may occur due to exogenous (e.g. UV light, cigarette smoke) or endogenous (e.g. inflammatory, free radical) factors.  More precisely, for $i \in \{0, 1, \ldots, n\}$, $g \in G$ and $s \in S$, let $\phi_{igs} > 0$ denote the mutation rate, and suppose that
\begin{equation}
    \label{eq:Poisson}
M_{igs} \sim \mathrm{Poisson}(\phi_{igs}),
\end{equation}
where $M_{igs}$ and $M_{i'g's'}$ are independent for $(i,g,s) \neq (i',g',s')$.  Further, to model the dependence of the mutation rate on the sample, gene and mutation type, we use a log-link function and let 
\begin{equation}
    \label{eq:loglink}
\log(\phi_{igs}) =  \mu_i + \log(\ell_g) + \lambda_g + \nu_s + \eta_{gs},
\end{equation} 
for $\mu_i, \lambda_{g},\nu_{s},\eta_{gs} \in \mathbb{R}$, where for identifiability we set $\eta_{gs_1} = 0$, for  some $s_1 \in S$ and all $g \in G$. The terms in our model can be interpreted as follows. First, $\mu_i$ is a sample-dependent parameter which corresponds to the \acrshort{bmr} of the sample. The offset $\log(\ell_g)$ accounts for a baseline mutation rate that is proportional to the length of a gene, so that a non-zero value of $\lambda_g$ corresponds to increased or decreased mutation rate relative to the \acrshort{bmr}. Other work has taken into account gene length in panel selection by rejecting genes beneath a given length threshold (for example in \citet{lyu_mutation_2018}). The parameters $\nu_s$ account for differences in frequency between mutation types. 
% As is standard to ensure model identifiability for interaction terms, we restrict the first 'column' of $\eta$ to be zero, i.e. for some $s_1 \in S$ we set $(\forall g \in G) \ \eta_{gs_1} = 0$.   

The model in \eqref{eq:Poisson} and \eqref{eq:loglink} (discounting the unseen test sample $i=0$) has $n + |S| + |G||S|$ free parameters and we have $n|G||S|$ independent observations. In principle we could attempt to fit our model directly using maximum likelihood estimation. However, we wish to exploit the fact that most genes do not play an active role in the development of a tumour, and so will experience mutations purely according to the \acrshort{bmr}. This corresponds to the parameters $\lambda_g$ and $\eta_{gs}$ being zero for many $g \in G$. We therefore include an $\ell_1$-penalisation term applied to the parameters $\lambda_g$ and $\eta_{gs}$ when fitting our model \citep{van_de_geer_high-dimensional_2008}. We do not penalise the parameters $\nu_s$ or $\mu_i$, since we expect differences in mutation rate between variant types and between samples. More precisely, writing $\mu = (\mu_1,\ldots, \mu_n)$, $\lambda = (\lambda_g :g \in G)$, $\nu = (\nu_s: s \in S)$ and $\eta = (\eta_{gs}: g\in G, s\in S)$, and given observations $M_{igs} = m_{igs}$, we let
\[
\mathcal{L}(\mu, \lambda, \nu, \eta) = \sum_{i = 1}^n \sum_{g \in G} \sum_{s \in S} \Bigl( \phi_{igs} - m_{igs} \log \phi_{igs} \Bigr) 
\]
be the negative log-likelihood of the model specified by \eqref{eq:Poisson} and \eqref{eq:loglink}. We then define
\begin{equation}
(\hat{\mu}, \hat{\lambda}, \hat{\nu}, \hat{\eta}) = \argmin_{\mu,\lambda, \nu, \eta} \Bigl\{ \mathcal{L}(\mu, \lambda, \nu, \eta) + \kappa_1 \Bigl(\sum_{g \in G} |\lambda_g|  +  \sum_{g \in G} \sum_{s \in S} |\eta_{gs}| \Bigr) \Bigr\},
\label{eq:genparams}
\end{equation}
where $\kappa_1 \geq 0$ is a tuning parameter that controls the number of non-zero components in $\hat{\lambda}$ and $\hat{\eta}$, which we choose using cross-validation. 
% KEEP We do not penalise main and interaction effects separately in order to have a unique solution, nor do we insist on hierarchical interaction effects (such as in \citet{lim_learning_2015}).
% \tcnote{Different penalty weights on $\lambda$ and $\eta$?}\\
% \jbnote{I think that now we've specified first column of $\eta$ is zero we no longer need to justify this - main effects and interaction terms are too closely intertwined to sensibly penalise separately. If reviewers ask, we can point to literature on interaction term penalisation which does not penalise separately, e.g. \citep{lim_learning_2015}}


\subsection{Proposed estimator \label{sec:linearestimator}}
We now attend to our main goal of estimating a given exome-wide biomarker for an unseen sample. Fix $\bar{S} \subseteq S$ and write $T = T_{0\bar{S}}$ to simplify notation. Recall that we wish to construct an estimator of $T$ that only depends on the mutation counts in a gene panel $P \subset G$, subject to a constraint on $\ell_P$. To that end, we consider estimators of the form\footnote{Note that our estimator may use the the full set $S$ of variant types, rather than just those in $\bar{S}$. In other words, our estimator may utilise information from every mutation, not just those that directly constitute the biomarker being predicted. This is important when estimating the mutation types in $\bar{S}$ are relatively scarce (e.g. tumour indel burden).}
\[
T(w) := \sum_{g \in G} \sum_{s \in S} w_{gs}M_{0gs},
\]
for $w \in \mathbb{R}^{|G|\times |S|}$. We choose $w$ to minimise the expected squared error of $T(w)$ based on the generative model above. Note that setting $w_{gs}= 1$ for $g \in G$ and $s \in \bar{S}$ and $w_{gs} = 0$ otherwise will give $T(w) = T$. 
However if, for a given $g$, we have $w_{gs} = 0$ for all $s \in S$, then our estimator does not depend on $g$ and therefore the gene does not need to be included in the panel. In order to produce a concise targeted gene panel (i.e. with many $w_{gs} = 0$), we penalise non-zero components of $w$. In order to improve predictive performance by eliminating shrinkage bias we define our final estimator via a refitting procedure. This will also be helpful when extending our procedure to accommodate panel design with predetermined genes.

Now, by \eqref{eq:Poisson}, we have that $\mathbb{E}M_{gs} = \var(M_{gs}) = \phi_{0gs}$, and it follows that the marginal expected squared error of $T(w)$ is
\begin{align}
\label{eq:MSE}
\mathbb{E}[\{T(w) - T\}^2] & = \mathrm{Var}(T(w)) + \mathrm{Var}(T) - 2\mathrm{Cov}(T(w), T) + \bigl[\mathbb{E}\{T(w) - T\}\bigr]^2  \nonumber \\
%\\ & = \sum_{g \in G}\sum_{s \in S}w_{gs}^2\mathrm{Var}(M_{gs}) + \sum_{g \in G}\sum_{s \in \bar{S}}\mathrm{Var}(M_{gs}) - 2\sum_{g \in G} \sum_{s \in \bar{S}} w_{gs}\mathrm{Var}(M_{gs})  \\
%& \hspace{60pt} + \Bigl(\sum_{g \in G}\sum_{s \in S}w_{gs}\mathbb{E}M_{gs} - \sum_{g \in G} \sum_{s \in \bar{S}}\mathbb{E}M_{gs} \Bigr)^2
% \\ & = \sum_{g \in G}\sum_{s \in S}w_{gs}^2\phi_{0gs}  + \sum_{g \in G}\sum_{s \in \bar{S}}\phi_{0gs} - 2\sum_{g \in G} \sum_{s \in \bar{S}} w_{gs}\phi_{0gs}   + \Bigl(\sum_{g \in G}\sum_{s \in S}w_{gs}\phi_{0gs} - \sum_{g \in G} \sum_{s \in \bar{S}}\phi_{0gs} \Bigr)^2 \\
& = \sum_{g \in G} \sum_{s \in \bar{S}}(1- w_{gs})^2\phi_{0gs} + \sum_{g \in G} \sum_{s \in S \setminus \bar{S}}w_{gs}^2\phi_{0gs}  \nonumber \\ & \hspace{150pt} + \Bigl(\sum_{g \in G}\sum_{s \in S}w_{gs}\phi_{0gs}
- \sum_{g \in G} \sum_{s \in \bar{S}}\phi_{0gs} \Bigr)^2.
\end{align} 
This error depends on the unknown parameters $\mu_0, \lambda_g, \nu_s$ and $\eta_{gs}$, the latter three of which are estimated as described in \eqref{eq:genparams}. 
% However, we have no estimate of $\mu_0$, and so attempt to formulate the problem in such a way as to have minimal dependence on $\mu_0$. 
To highlight the dependence on $\mu_0$, write $\hat{\phi}_{0gs} = \ell_g\exp(\hat{\lambda}_g + \hat{\nu}_s + \hat{\eta}_{gs})$, and define
\[
p_{gs}  := \frac{\hat{\phi}_{0gs}}{\sum_{g' \in G} \sum_{s' \in \bar{S}} \hat{\phi}_{0g's'}} = \frac{\ell_g \exp(\hat{\lambda}_g + \hat{\nu}_s + \hat{\eta}_{gs})}{\sum_{g'\in G} \sum_{s'\in \bar{S}} \ell_{g'} \exp(\hat{\lambda}_{g'} + \hat{\nu}_{s'} + \hat{\eta}_{g's'})}.
\]
Then let
\[
f(w) := \sum_{g \in G}\sum_{s \in \bar{S}} p_{gs}(1-w_{gs})^2  + \sum_{g \in G}\sum_{ s\in S \setminus \bar{S}}p_{gs} w_{gs}^2 + K(\mu_0)\big( 1 - \sum_{g \in G}\sum_{s \in S}  p_{gs}w_{gs} \big)^2,
\]
where $K(\mu_0) = \exp(\mu_0)\sum_{g \in G}\sum_{s \in \bar{S}}l_{g}\exp(\hat{\lambda}_g + \hat{\nu}_s + \hat{\eta}_{gs})$. Notice that $f$ is a rescaled version of the error  $\mathbb{E}[\{T(w)-T\}^2]$, where the true parameters $(\lambda, \nu, \eta)$ have been replaced by the estimates $(\hat{\lambda}, \hat{\nu}, \hat{\eta})$. Moreover, $f$ only depends on $\mu_0$ via the $K(\mu_0)$ term. The quantity $K(\mu_0)$ can be interpreted as a penalty factor controlling the \emph{bias} of our estimator. For example, we may insist that the squared bias term $(1 - \sum_{g \in G}\sum_{s \in S}  p_{gs}w_{gs})^2$ is zero (by setting $K(\mu_0) = \infty$). In practice, we propose to choose the penalty $K$ based on the training data; see Section \ref{sec:practicalconsiderations}. 

At this point $f(w)$ is minimised by choosing $w$ to be such that $w_{gs}= 1$ for all $g\in G, s\in\bar{S}$. As mentioned above, in order to form a concise panel while optimising predictive performance we solve this minimisation while constraining the cost of sequencing the genes used in the estimation. For a given $w$, an appropriate cost is given by
\[
\|w\|_{G,0} := \sum_{g \in G} \ell_g\mathbbm{1}\{w_{gs} \neq 0 \ \mathrm{for \ some \ } s \in S \} .
\]
This choice acknowledges that the cost of a panel is roughly proportional to the length of the region of genomic space sequenced, and that once a gene has been sequenced for one mutation type there is no need to sequence again for other mutation types. Given a cost restriction $L$, our optimisation problem is to minimise $f(w)$ such that $\|w\|_{G,0} \leq L$. In practice this problem is non-convex and so computationally infeasible. As is common in high-dimensional optimisation problems, we consider a convex relaxation of this problem.  More precisely, let $\|w\|_{G,1} = \sum_{g \in G} \ell_g \|w_g\|_2$, where $w_g$ here refers to the vector $(w_{g1}, \ldots, w_{g|S|})^T$ and $\|.\|_2$ is the Euclidean norm.  Let\footnote{We will assume that $\hat{w}^{\text{first-fit}}$, and the later defined $\hat{w}^{\text{refit}}$, are unique; see Section \ref{sec:practicalconsiderations}.}
\begin{equation}
    \label{eq:wfirstfit}
\hat{w}^{\text{first-fit}} \in \argmin\limits_{w} \bigl\{ f(w) +\kappa_2\|w\|_{G,1} \bigr\},
\end{equation}
where $\kappa_2 \geq 0$ is chosen to specify the level of cost penalisation, and will determine the size of panel selected.

Now, we could use $T(\hat{w}^{\text{first-fit}})$ as our estimator of $T$, and indeed we will include the performance of this approach in our numerical comparison in Section~\ref{sec:experimentalresults}. However our final proposed estimator is motivated by the refitted \acrshort{lasso} procedure \citep[see][]{chzhen_lasso_2019}, which in the case of standard linear regression can improve predictive performance by reducing bias. 
%At this point we could define our estimator as $\hat{T} := T(\hat{w}^{\text{first-fit}})$, and indeed we will show results for such an estimator in subsequent sections (referred to as the 'first-fit $\hat{T}$ estimator'). However our final proposed estimator is motivated by the refitted lasso procedure \citep[see][]{chzhen_lasso_2019}, which in the case of standard linear regression can improve predictive performance by reducing shrinkage bias. Informally, we allow our first estimator to perform variable selection only and fit a new estimator on the selected covariates. 
Indeed, let $\hat{P} := \{g \in G: \ \|\hat{w}^{\text{first-fit}}_g\|_2 > 0 \}$ be the panel selected by our procedure above, and for $P\subseteq G$ let
% and, for $P\subseteq G$, let
% \[
% f_{P}(w) := \sum_{g \in P}\sum_{s \in \bar{S}} p_{gs}(1-w_{gs})^2  + \sum_{g \in P }\sum_{ s\in S \setminus \bar{S}}p_{gs} w_{gs}^2 + K(\mu_0)\big( 1 - \sum_{g \in P}\sum_{s \in S}  p_{gs}w_{gs} \big)^2
% \]
% be the portion of $f$ corresponding to the genes in $P$, and
\[
W_{P} := \{ w \in \mathbb{R}^{|G| \times |S|} : w_g = (0, \ldots, 0) \ \ \text{for} \ \ g \in G\setminus P \}.
\] 
Finally, then, our proposed estimator of $T$ is given by $\hat{T} := T(\hat{w}^{\text{refit}}),$ where  
\begin{equation} 
\label{eq:wrefit}
\hat{w}^{\text{refit}} \in  \argmin\limits_{w \in W_{\hat{P}}} \bigl\{f(w)\bigr\}.
\end{equation}
% Refitting as described here has two empirical effects on our coefficients. Firstly, the coefficients of $\hat{w}^{\text{refit}}$ are less clustered towards zero than for $\hat{w}^{\text{first-fit}}$. Secondly, they are more uniform. When refitting is used, our estimator $\hat{T}$ becomes much more similar to a density-based estimator an overall scaling factor to calibrate predictions based on it. This learned scaling will give an advantage over other density-based estimates when measuring absolute prediction accuracy (as opposed to correlation), which we do in Section~\ref{sec:experimentalresults}.


\subsection{Panel augmentation \label{sec:panelaugmentation}}
When designing gene panels in practice, there are often a variety of factors contributing to the choice of which genes are included. For example, a gene may be included due to its relevance to immune response or its known association with a particular cancer type. If this is the case, measurements for these genes will be made regardless of their utility for predicting exome-wide biomarkers. When implementing our methodology, therefore, there is no additional cost to incorporate observations from these genes into our prediction if they will be helpful. Conversely researchers may wish to exclude genes from a panel, or at least from actively contributing to the estimation of a biomarker, for instance due to technical difficulties in sequencing a particular gene. 

We can accommodate these restrictions by altering the structure of our regularisation penalty in~\eqref{eq:wfirstfit}.  More precisely, given (disjoint) $P_0, Q_0 \subseteq G$ that have been separately identified for inclusion and exclusion from a panel respectively, we can replace $\hat{w}^{\text{first-fit}}$ in~\eqref{eq:wfirstfit} with 
\begin{equation} \label{eq:augment}
\hat{w}_{P_0, Q_0}^{\text{first-fit}} \in \argmin\limits_{w \in W_{G \setminus Q_0}} \bigl\{ f(w) + \kappa_2 \sum_{g \in G\setminus P_0} l_g \|w_g\|_2 \bigr\}.  
\end{equation}
Excluding the elements of $P_0$ from the penalty term guarantees that $\hat{w}_{P_0, Q_0}^{\text{first-fit}} \neq 0$ for $g \in P_0$, while restricting our optimisation to $W_{G \setminus Q_0}$ excludes the genes in $Q_0$. This has the effect of augmenting the predetermined panel $P_0$ with further genes selected to improve the predictive performance. We may then perform refitting as described above. We demonstrate this procedure by augmenting a commercially available gene panel in Section~\ref{sec:augmentation}.

\subsection{Practical considerations \label{sec:practicalconsiderations}}
Our first consideration here concerns the choice of the tuning parameter $\kappa_1$ in $\eqref{eq:genparams}$. We use 10-fold cross-validation to select $\kappa_1$, as is common for the \acrshort{lasso} estimator in generalised linear regression and for which there exists a rich theory and computational toolkit \citep{michoel_natural_2016, friedman_glmnet_2020}. To highlight one important aspect of our cross-validation procedure, recall that we consider the observations $M_{igs}$ as independent across the sample index $i \in \{1, \ldots, n\}$, the gene $g\in G$ and the mutation type $s\in S$. Our cross-validation method therefore involves splitting the entire set $\{(i,g,s): i = 1, \ldots, n, g \in G, s \in S\}$ of size $n|G||S|$ (as opposed to simply the sample set $\{1, \ldots, n\}$) into 10 folds uniformly at random.

The estimated coefficients in \eqref{eq:wfirstfit} depend on the choice of $K(\mu_0)$ and $\kappa_2$.  As mentioned above, we could set $K(\mu_0) = \infty$ to give an unbiased estimator, however in practice we found that a finite choice of $K(\mu_0)$ leads to improved predictive performance. In particular, we recommend using $K(\mu_0) = K(\max_{i=1,\ldots, n}\{\hat{\mu}_i\})$, with $\hat{\mu}_i = \log(\hat{T_i}/\sum_{g,s}\ell_g\exp(\hat{\lambda}_g + \hat{\nu}_s + \hat{\eta}_{gs}))$, so that the penalisation is broadly in proportion with the values observed in the training dataset. The tuning parameter $\kappa_2$ controls the size of the gene panel selected in \eqref{eq:wfirstfit}: for example, we set $\kappa_2 = \max \{\kappa: \ \ell_{\hat{P}} \leq L \}$ in order to produce a panel of length less than or equal to $L$.


The convex optimisation problem in \eqref{eq:wfirstfit} can be solved by any method designed for the group \acrshort{lasso}; see, for example, \citet{yang_fast_2015}. In our experiments below we used the \texttt{gglasso} R package \citep{yang_gglasso_2020}. Note also that the solutions to \eqref{eq:wfirstfit} and \eqref{eq:wrefit} are unique; see \citet[Theorem~1]{roth_group-lasso_2008}.

Finally we describe a heuristic procedure for producing prediction interval around our estimates. In particular, for a given confidence level $\alpha$, we aim to find an interval $[\hat{T}_{\mathrm{L}}, \hat{T}_{\mathrm{U}}]$ such that $\mathbb{P}\bigl(\hat{T}_{\mathrm{L}} \leq T \leq \hat{T}_{\mathrm{U}}\bigr) \geq 1- \alpha.$  To that end, let $t_\alpha := \mathbb{E}\{(\hat{T} - T)^2\}/\alpha$, then by Markov's inequality, we have that  $ \mathbb{P}(|\hat{T} - T|^2 \geq t_\alpha) \leq \alpha$. It follows that $[\hat{T} - t_\alpha^{1/2} , \hat{T}+ t_\alpha^{1/2}]$ is a $(1-\alpha)$-prediction interval for $T$. Of course, $t_\alpha$ is unknown since it depends on the mean squared error $\mathbb{E}\{(\hat{T}-T)^2\}$. For our refitted estimator, this is determined by \eqref{eq:MSE} with $w = \hat{w}^{\text{refit}}$. Here, the parameters $\lambda, \eta, \nu$ and $\mu_0$ are unknown. Our heuristic approach is to plug-in the fitted estimates $\hat{\lambda}, \hat{\eta}, \hat{\nu}$ and estimate $\mu_0$ with $\log(\hat{T}/\sum_{g,s}\ell_g\exp(\hat{\lambda}_g + \hat{\nu}_s + \hat{\eta}_{gs}))$. This is not, therefore, an exact $(1-\alpha)$-prediction interval for $T$; however, we will see in our experimental results in Sections~\ref{sec:tmb}~and~\ref{sec:indel} that in practice this approach does provide valid $90\%$ prediction intervals. \jbnote{Come back to this!}.  

\section{Experimental results \label{sec:experimentalresults}}
In this section we demonstrate the practical performance of our proposal using the dataset from \citet{campbell_distinct_2016}, which we introduced in Section~\ref{sec:dataterminology}. We focus on prediction of \acrshort{tmb} and \acrshort{tib}, due to their stature as established and recently proposed biomarkers respectively. We aim to show state-of-the-art performance for prediction of \acrshort{tmb}, and to provide proof-of-principle results for estimating \acrshort{tib}, for which there is little prior art.

Since we are only looking to produce estimators for \acrshort{tmb} and \acrshort{tib}, we group mutations in two categories so that $|S|=2$. These are \textit{a)} indel mutations and \textit{b)} all other non-synonymous mutations. This has a simplifying effect, both in terms of the presentation of our results and the computational power required to fit the generative model. We split the dataset into training, validation and test sets containing $n=800, n_{val} = 172$ and $n_{test} = 172$ samples respectively. We have mutation information for $|G| = 17681$ genes. For the samples forming our training set, the average \acrshort{tmb} is 267 and the average \acrshort{tib} is 9.91. 

In the following sections, we describe the fitted generative model and investigate the extent to its structure is reasonable, then separately address the performance of derived estimators of \acrshort{tmb} and \acrshort{tib}. We conclude with a case study into augmenting the Foundation~1 panel to predict \acrshort{tmb} and \acrshort{tib}.

\subsection{Generative model fit and validation \label{sec:genmodelfit}}
The first step in our analysis is to fit the model proposed in Section \ref{sec:genmodel} using only the training dataset. In particular, we obtain estimates of the model parameters using equation~\eqref{eq:genparams}, where the tuning parameter $\kappa_1$ is determined using 10-fold cross-validation as described in Section~\ref{sec:practicalconsiderations}.  The results of the cross-validation procedure are presented in Figure~\ref{fig:genmodelstats}. The best choice of $\kappa_1$ produces estimates of $\lambda$ and $\eta$ with $40.2\%$ and $76.2\%$ sparsity respectively, i.e. that proportion of their components are estimated to be exactly zero. We plot $\hat{\lambda}$ and $\hat{\eta}$ for this value of $\kappa_1$ in Figures \ref{fig:manhat_plot} and \ref{fig:manhat_plot_indel}. Genes with $\hat{\lambda}_g = 0$ are interpreted as mutating according to the background mutation rate, and genes with $\hat{\eta}_{\text{g,indel}} = 0$ are interpreted as having no specific selection pressure for or against indel mutations. In Figures \ref{fig:manhat_plot} and \ref{fig:manhat_plot_indel} we highlight genes with large parameter estimates that also have known biological relevance in oncology; see Section~\ref{sec:conclusion} for further discussion.   
\begin{figure}[htbp]
\centering
\vspace*{-5mm}
\includegraphics[width=4in]{GenModelCV.png}
\vspace*{0mm}
\caption{The average deviance (with one standard deviation) across the 10 folds in our cross-validation procedure plotted against $\log(\kappa_1)$. The minimum deviance is highlighted red.\label{fig:genmodelstats}}
\vspace*{-2mm}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{manhat_plot.png}
\vspace*{0mm}
\caption{Manhattan plot of fitted parameters $\hat{\lambda}_g$ and their associated genes' chromosomal locations. The genes with the five largest positive parameter estimates are labelled. \label{fig:manhat_plot}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{manhat_plot_indel.png}
\caption{Manhattan plot of fitted parameters $\hat{\eta}_{g,\text{indel}}$ and their associated genes' chromosomal locations. The five largest positive and negative genes are labelled.  \label{fig:manhat_plot_indel}}
\end{figure}
In order to provide some validation of the model formulation in \eqref{eq:loglink}, we compare our model with the following alternatives: 
%performed \jbnote{'deviance comparisons'} on our model against a range of alternatives. 
\begin{enumerate}[(i)]
\item Saturated model: each observation has an associated free parameter (i.e. $\phi_{igs} > 0$ in \eqref{eq:Poisson} is unrestricted);
\item No sample-specific effects:  $\mu_i = 0$ for all $i \in \{1,\ldots, n\}$;
\item No gene-specific effects: $\lambda_g = \eta_{gs} = 0$ for all $g \in G$ and $s\in S$; 
\item No gene/mutation type interactions: $\eta_{gs} =0$ for all $g \in G$ and $s\in S$.
\end{enumerate}

In Table~\ref{table:goodnessoffit} we present the residual deviance and the residual degrees of freedom between our model and each of the models above. First note that the decrease in deviance between our model and model (i) is very small relative to the additional degrees of freedom associated with the saturated model; this suggests that our model is more appropriate. On the other hand, when comparing our model with the alternatives in (ii)-(iv), the decrease in deviance large relative to the increase in degrees of freedom, and our model is therefore preferred.    

%Other than these restrictions we fitted each submodel in exactly the same way as described in Section \ref{sec:genmodel}. 
%The results of these comparisons are given in Table \ref{table:goodnessoffit}. We see that making any of the restrictions discussed severely inhibits the explanatory power of the generative model, even when allowing for the residual degrees of freedom in the larger model. Furthermore a saturated model does not offer nearly enough improved prediction to justify the large increase in degrees of freedom. 

%We see strong evidence that it is necessary to incorporate both sample-specific and gene-specific parameters into a model of mutation, but that a saturated model does not nearly give enough improvement in prediction to justify its vastly increased degrees of freedom. We find $p$-values indistinguishable from 0 and 1 - these should not be interpreted too strongly, as various high-dimensional and model selection effects can come to bear when quantifying such comparisons. 

%Modern methods for testing goodness-of-fit of high dimensional generalised linear models such as proposed by \citet{jankova_goodness--fit_2019} are discussed in Appendix \ref{app:goodnessoffit}, including a proposal for using such methodology to predict genes containing hotspot codons. 

\begin{table}[ht]
\begin{center}
\caption{Model comparisons on the basis of residual deviance statistics. \label{table:goodnessoffit}}
\begin{tabular}{ || c || c | c | c  | c ||}
\hline
\hline
Comparison  & Residual  & Residual Degrees & dev/df & $p$-value \\
Model            & Deviance (dev)             & of Freedom (df)  &        &           \\
\hline
\hline
(i) &  $7.50 \times 10^5$  & $2.80\times 10^7$  &  $2.68 \times 10^{-2}$   &  1.00 \\
\hline
(ii)  &  $1.61 \times 10^5$  &  $8.00 \times 10^2$  &  $2.01 \times 10^2$ &  0.00\\
(iii)  &  $2.00 \times 10^5$ & $3.49 \times 10^4$  & $5.74 \times 10^0$&  0.00 \\
(iv) & $1.09 \times 10^5$ & $1.75 \times 10^4$ & $1.09 \times 10^1$ & 0.00 \\
\hline
\hline
\end{tabular}
\vspace*{-10mm}
\end{center}
\end{table}


\subsection{Predicting tumour mutation burden \label{sec:tmb}}
We now demonstrate the excellent practical performance of our procedure for estimating \acrshort{tmb}. Recall that \acrshort{tmb} is given by equation~\eqref{eq:biomarker} with $\bar{S}$ being the set of all non-synonymous mutation types. First it is shown that our method can indeed select gene panels of size specified by the practitioner and that good predictions can be made even with small panel sizes (i.e.~$\leq$ 1Mb). We then compare the performance of our proposal with state-of-the-art estimation procedures based on a number of widely used gene panels.

In order to evaluate the predictive performance of an estimator we calculate the $R^2$ score on the validation data. More precisely, given predictions of \acrshort{tmb}, $\hat{t}_1, \ldots, \hat{t}_{n_{val}}$, for the observations in the validation set with true \acrshort{tmb} scores $t_1, \ldots, t_{n_{val}}$. Let $\bar{t} := \frac{1}{n_{val}} \sum_{i=1}^{n_{val}} t_i$, and let 
    \[
    R^2 = 1- \frac{\sum_{i =1}^{n_{val}}(t_i - \hat{t}_i)^2}{\sum_{i = 1}^{n_{val}}(t_i - \bar{t})^2}. 
    \]
 While it's not the focus of our approach, other works have been interested in separating tumours into two classes (High \acrshort{tmb}, Low \acrshort{tmb}) \citep{buttner_implementing_2019, wu_designing_2019}. We therefore also report the estimated area under the precision-recall curve (\acrshort{auprc}) for a classifier based on our approach. The \acrshort{auprc} is calculated as follows: first, in line with major clinical studies \citep[see, for example,][]{hellmann_nivolumab_2018, ramalingam_abstract_2018}, the true class membership of a tumour is defined according to whether it has $t^* := 300$ or more exome mutations (approximately 10 Mut/Mb). This gives 53 (30.8\%) tumours with high \acrshort{tmb} and 119 (69.2\%) with low \acrshort{tmb} in the validation set. Now, for a cutoff $t \geq 0$, let 
 \[
 p(t) := \frac{\sum_{i=1}^{n_{val}} \mathbbm{1}_{\{\hat{t}_i \geq t, \ t_i \geq t^*\}}}{\sum_{i=1}^{n_{val}} \mathbbm{1}_{\{\hat{t}_i \geq t\}}}; \quad  r(t) := \frac{\sum_{i=1}^{n_{val}} \mathbbm{1}_{\{\hat{t}_i \geq t, \ t_i \geq t^*\}}}{\sum_{i=1}^{n_{val}} \mathbbm{1}_{\{t_i \geq t^*\}}}
 \]
 be the estimated (over the validation set) precision and recall, respectively, of a classifier that assigns a tumour to the High \acrshort{tmb} class if its estimated \acrshort{tmb} is greater than or equal to $t$.  The precision-recall curve is $\{(r(t),p(t)): t \in [0, \infty)\}$. Note that a perfect classifier achieves a \acrshort{auprc} of 1, whereas a random guess in this case would have an average \acrshort{auprc} of 0.308 (the prevalence of the High \acrshort{tmb} class).
% although standardisation of TMB thresholds is an open endeavour \citep{chang_p086_2018, stenzinger_tumor_2019}. 
%$R^2$ score and Area Under Precision/Recall Curve (\acrshort{auprc}). 

%For predictions $\hat{t}_i \ (i \in \{1,\ldots, n_{val}\})$, where $n_{val}$ is the number of validation samples, of true TMB scores $t_i$ with mean $\bar{t}_i$, we take  

\begin{figure}[ht]
\centering
\vspace{-5mm}
\includegraphics[width=6in]{EstimatorPerformance.png}
\vspace*{-5mm}
\caption{Performance of our first-fit and refitted estimators of \acrshort{tmb} as the selected panel size varies. \textbf{Left}: $R^2$, \textbf{Right}: \acrshort{auprc}. \label{fig:EstimatorPerformance}}
\vspace*{-2mm}
\end{figure}

To estimate \acrshort{tmb} we apply our procedure in Section~\ref{sec:linearestimator} with $\bar{S} = S$, where the model parameters are estimated as described in Section~\ref{sec:genmodelfit}. In Figure~\ref{fig:EstimatorPerformance}, we present the $R^2$ and \acrshort{auprc} for the first-fit and refitted estimators (see~\eqref{eq:wfirstfit} and~\eqref{eq:wrefit}, respectively) as the selected panel size varies from 0Mb to 2Mb in length. We see that we obtain a more accurate prediction of \acrshort{tmb}, both in terms of regression and classification, as the panel size increases, and that good estimation is possible even with very small panels (as low as 0.25Mb). %For panels less than length 0.25Mb, estimation of \acrshort{tmb} is challenging. 
Finally, as expected, the refitted estimator slightly outperforms the first-fit estimator. 

%We see elevated performance in our refitted estimator compared to the first-fit, and a sharp increase in predictive performance up to panel size around 0.25Mb, and then stably increasing performance for larger panel sizes. We see a similar trend with an even sharper corner when it comes to classification performance, as shown in Figure~\ref{fig:TvLvRF_class}.


% \begin{figure}[htbp]
% \centering
% \includegraphics[width=6in]{ComparisonWithLassos.png}
% \caption{\jbnote{Get rid of correlation. Separate into three plots (second two may well disappear}\textbf{A}: Performance of regression to predict \acrshort{tmb} at different panel sizes, utilising a range of panel selection/prediction techniques (linear/generalised lasso and random forests). Regression metrics $R^2$ and correlation are shown on our validation dataset. \textbf{B}: The same comparisons, this time for classification, using area under receiver-operator characteristic and precision-recall curves as predictive metrics. \label{fig:TvLvPwithCRP}}
% \end{figure}

We now compare our method with state-of-the-art estimators applied to commonly used gene panels. The four next generation sequencing panels, Foundation~One \citep{frampton_development_2013}, MSK-IMPACT \citep{cheng_memorial_2015}, TST-170 \citep{heydt_evaluation_2018} and TSO-500 \citep{pestinger_use_2020}, are chosen for their relevance to \acrshort{tmb}. For each panel $P \subseteq G$, we use four different methods to predict \acrshort{tmb}:
\begin{enumerate}[(i)]
 \item Our refitted estimator applied to the panel $P$: we estimate \acrshort{tmb} using $T(\hat{w}_P)$, where $\hat{w}_P \in \argmin_{w \in W_P} \{f(w)\}$. 
 \item \acrshort{ectmb}: the procedure proposed by \citet{yao_ectmb_2020}.
%  \item A ridge regression model, learned on the training dataset and on the genes in the panel, and optimised by cross-validation.
 \item A count estimator: we estimate \acrshort{tmb} with $\frac{\ell_G}{\ell_P} \sum_{g \in P} \sum_{s \in S}M_{0gs}$, i.e. simply rescaling the mutation burden in the genes of $P$. 
 \item A linear model: we estimate \acrshort{tmb} with ordinary linear regression of \acrshort{tmb} against $\{M_{0gs}: g \in P, s \in S\}$.
\end{enumerate}
We include our refitted estimator applied to the panel $P$ in order to demonstrate the utility of our approach, even with a prespecified panel. The second approach is a recent method, based on a negative binomial model. The third and fourth are simple and standard practical procedures for inferring \acrshort{tmb} from targeted gene panels. 


\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{commercial_comparison.png}
\vspace*{-5mm}
\caption{The performance of our \acrshort{tmb} estimator in comparison to existing approaches. \textbf{Left}: $R^2$, \textbf{Right}: \acrshort{auprc}. \label{fig:commercial_comparison}}
\vspace*{-2mm}
\end{figure}

We present results of these comparisons in  Figure~\ref{fig:commercial_comparison}. For each of the four panels considered here, we see that our refitted estimator applied to the panel outperforms all existing approaches in terms of regression performance, demonstrating that making predictions based on our generative model has tangible benefit. Furthermore, we see that by first selecting a panel based on the data, we are able to improve predictive accuracy even further. For instance, in comparison to predictions based on the Foundation 1 panel, our procedure with a selected panel of the same size achieves an $R^2$ of 0.88. The best available existing method based on the panel, in this case the count estimator, has an $R^2$ of 0.77. Similar results are seen for all panels included here.   In terms of classification performance, we see that good predictions based on given panels are possible using the count estimator and our refitted estimator applied to the panel. However, we see that by selecting a panel our approach is more accurate. In particular, even with the smallest panel size presented in Figure~\ref{fig:commercial_comparison}, our method is competitive with those based on the large panels.  

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{OneMBPanelPredictions.png}
\vspace*{-5mm}
\caption{Prediction of TMB on the test dataset. Dashed blue (diagonal) line represents perfect prediction, and the black dashed lines indicate true and predicted TMB values of 300. Red shaded area delineates region for which 90\% prediction intervals contain truth.
% \textbf{Top Left}: Refitted $\hat{T}$ Estimator, \textbf{Top Right}: ecTMB Estimator, \textbf{Bottom Left}: Count estimator, \textbf{Bottom Right}: Linear Regression Estimator.  
\label{fig:OneMBPredictions}}
\vspace*{-2mm}
\end{figure} 

Finally in this section we demonstrate the practical performance of our method using the test set, which until this point has been reserved. Based on the validation results above, we take the panel of size ~1Mb selected by our procedure and use our refitted estimator on that panel to predict \acrshort{tmb} for the samples in the test set. For comparison, we also present predictions from \acrshort{ectmb}, count-based and linear regression estimators applied to the same panel. In Figure~\ref{fig:OneMBPredictions} we see that our procedure performs very well; we obtain an $R^2$ value (on the test data) 0.89.  The other methods perform less well, with $R^2$ values of 0.72 (\acrshort{ectmb}), -19.2 (count) and 0.67 (linear). The count estimator here gives predictions which are reasonably well correlated to the true values of \acrshort{tmb} but are positively biased. This is as expected, since our selection procedure tends to favour genes with higher overall mutation rates. We also plot the region for which our procedure for generating heuristic prediction intervals (as described in Section~\ref{sec:practicalconsiderations}) is successful. More precisely, we shade all true \acrshort{tmb}/predicted \acrshort{tmb} pairs for which a 90\% prediction interval generated around the estimate \acrshort{tmb} value would include the true value. We find in this case that 160 (93.0\%) of the points in the test set fall within this region, a good sign that our prediction intervals are performing as expected.


% We see that, as expected, the count-based estimator ($R^2=-19.2$) drastically overestimates TMB, while linear regression suffers from overfitting ($R^2 = 0.67$). Estimates from the ecTMB procedure do fairly well ($R^2 = 0.72$) aside from for samples with low true TMB. 



%We see, perhaps surprisingly, that for the commercial panels ridge regression models in general performed the most poorly, despite having access within their parameter space to both of the other models. This  emphasises the issues that linear models constructed without proper justification face from overfitting, even when regularisation is introduced. We see in general that in general the density and $\hat{T}$ estimators perform similarly when measured by \acrshort{auprc}. No commercial panel achieves performance in either metric that exceeds that of a $\hat{T}$-selected panel of the same size, and particularly for classification $\hat{T}$-selected panels of far smaller size seem to produce results comparable to those of the predefined panels. We see this as a positive sign that our method has utility both for selecting \acrshort{tmb} predictive panels, and for improving the predictions of preselected panels.



% \begin{figure}[htbp]
% \centering
% \includegraphics[width=4in]{commercial_classification.png}
% \caption{\jbnote{cut off at 0.8, move legend to caption?}\label{fig:commercial_classification}}
% \end{figure}

% % \begin{figure}
% % \centering
% % \includegraphics[width=6in]{panel_comparisons.png}
% % \caption{\jbnote{Get rid of correlation. Separate into three plots.}Comparing the predictive performance of gene panels (of varying size) selected by our methodology to commercially available gene panels. \textbf{A}: For prediction of \acrshort{tmb}, we give $R^2$ and correlation metrics. Points refer to commercial panels, with regression performed via three techniques. Lines refer to first-fit and refitted $\hat{T}$ estimators. \textbf{B}: The same comparisons for classification performance (measured by area under ROC curve and area under precision/recall curve). \label{fig:PanelComparisons}}
% % \end{figure}
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=4in]{commercial_regression_ectmb.png}
% \caption{\label{fig:commercial_regression}}
% \end{figure}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=4in]{commercial_classification_ectmb.png}
% \caption{\jbnote{cut off at 0.8, move legend to caption?}\label{fig:commercial_classification}}
% \end{figure}


\subsection{Predicting tumour indel burden \label{sec:indel}}
In this section we demonstrate how our method can be used to estimate \acrshort{tib}. 
This is more challenging than estimating \acrshort{tmb} due to the low abundance of indel mutations relative to other variant types. Indeed, in contrast to the previous section, we are not aware of any existing methods designed to estimate \acrshort{tib} from targeted gene panels. We therefore investigate the performance of our method across a much wider range of panel sizes in order to ascertain the feasibility of targeted \acrshort{tib} estimation. Our results demonstrate that accurate classification of \acrshort{tib} status is still possible, even though estimating the precise value of \acrshort{tib} is difficult with small panels.  

We let $S_{\text{indel}}$ be the set of all frameshift insertion and deletion mutations, and apply our method introduced in Section~\ref{sec:linearestimator} with $\bar{S} = S_{\text{indel}}$. As in the previous section, we assess regression and classification performance via $R^2$ and \acrshort{auprc}, respectively, where in this case tumours are separated into two classes: High \acrshort{tib} (more than 11 indel mutations) and Low \acrshort{tib} (otherwise). This gives 43 (25.0\%) tumours in the High \acrshort{tib} class in the validation dataset. 

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{indel_panel_stats.png}
\vspace*{-5mm}
\caption{Performance of our first-fit and refitted estimators of \acrshort{tib} as the selected panel size varies. \textbf{Left}: $R^2$, \textbf{Right}: \acrshort{auprc}. \label{fig:indelstatsplot}}
\vspace*{-2mm}
\end{figure}

The results are presented in Figure \ref{fig:indelstatsplot}. We comment first on the regression performance: as expected, we see that the $R^2$ values for our first-fit and refitted estimators in this case are much lower than what we achieved in estimating \acrshort{tmb}. Perhaps surprisingly, the predictive performance of both estimators very slightly decreases as the panel size increases in the range 0.5-3.0 Mb; the refitted approach then improves for larger panel sizes. On the other hand, we see that the classification performance is impressive; our refitted method achieves an \acrshort{auprc} of 0.87 even with panel of size of~1Mb. 

\begin{figure}[htbp]
\centering
\includegraphics[width=6in]{indel_predictions_figure.png}
\vspace*{-5mm}
\caption{Prediction of TIB on the test dataset. Dashed blue (diagonal) line represents perfect prediction, and the grey dashed lines indicate true and predicted TIB values of 11. Red shaded area delineates region for which 90\% prediction intervals contain truth.
\label{fig:indel_predictions_figure}}
\vspace*{-2mm}
\end{figure} 

As in the previous section, we now asses the performance on the test set of our refitted estimator of \acrshort{tib} applied to a selected panel of size~1Mb.   %and in Figure~\ref{fig:indel_predictions_figure} display the predictions of \acrshort{tib} made by the refitted estimator. 
In this case, we compare with a count estimator, which scales the total non-synonymous mutation count across the panel by the ratio of the length of the panel to that of the entire exome, and also by the relative frequency of indel mutations versus all non-synonymous mutations in the training dataset;~i.e. 
\[
\frac{\ell_G}{\ell_{P}} \frac{\sum_{i =1}^{n} \sum_{g \in G} \sum_{s \in S_{\text{indel}}} M_{igs}}{\sum_{i=1}^{n}\sum_{g \in G} \sum_{s \in S} M_{igs}}\sum_{g \in P} \sum_{s \in S} M_{0gs}.
\]
The results are presented in Figure~\ref{fig:indel_predictions_figure}. Our refitted $\hat{T}$ estimates give an $R^2$ value of 0.47 on the test set, while the count estimator again suffers from positive bias and has an $R^2$ value of -9.9. Again we include a 90\% prediction interval success region. In this case we find that 166 (96.5\%) of test set points fall within the success region.

%Since there is no facility within the ecTMB procedure to predict \acrshort{tib} we do not compare with it. No linear regression procedure was found to produce estimates of \acrshort{tib} more accurate than ours (for more details see Supplementary Information \jbnote{?}). \jbnote{Exact performance of prediction intervals}.



%This elevated performance on the test set compared to the decrease in performance seen around panel sizes of 1~Mb on the validation set leads us to believe the local minimum in validation set performance is an anomalous artifact of the validation dataset (this is also borne out by the classification performance on the validation set being very high). 



\subsection{A panel-augmentation case study \label{sec:augmentation}}
As discussed in Section \ref{sec:panelaugmentation}, we may wish to include genes from a given panel, but use our methodology to augment the panel to include a small set of genes in order to obtain better predictions of \acrshort{tmb}. In this section we demonstrate how this can be done starting with the Foundation~1 panel.  The Foundation~1 panel contains 324 genes (approximately 0.8Mb) - we will see that, by increasing the panel to 1Mb in size using our proposal, we can significantly improve the predictive performance. 

\begin{table}[ht]
\begin{center}
\caption{Predictive performance of models on Foundation~1 (0.8Mb) versus augmented Foundation~1 (1.0Mb) panels. \label{table:augpanel}}
\begin{tabular}{ || c || c | c || c | c ||}
\hline
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c ||}{Regression ($R^2$)} & \multicolumn{2}{c||}{Classification (AUPRC)}  \\
\cline{2-5}
  & Foundation  & Augmented  & Foundation  & Augmented    \\
 \hline
Refitted $\hat{T}$ & 0.69 & 0.82 & 0.80  &  0.91       \\
ecTMB  & 0.76 & 0.80 & 0.82 & 0.79  \\
Count & 0.61 & -0.54 & 0.79 & 0.91 \\
Linear & 0.58 & 0.62 & 0.73  & 0.84 \\
\hline
\hline

\end{tabular}
\vspace*{-5mm}
\end{center}
\end{table}

We apply the augmentation method described in Section~\ref{sec:panelaugmentation}, with $P_0$ taken to be Foundation~1 genes and set $Q_0$ to be empty.  The genes added to the panel are determined by the first-fit estimator in equation~\eqref{eq:augment}. To evaluate the performance, we then apply the refitted estimator on the test dataset, after selecting the augmented panel of size 1Mb. For comparison, we apply our refitted estimator to the Foundation~1 panel directly. We also present the results obtained by the other estimators described above, both before and after the panel augmentation, in Table~\ref{table:augpanel}. We find that the augmented panel produces enhanced predictive performance under the refitted $\hat{T}$ estimator in regression and classification. The refitted estimator provides better estimates than any other model on the augmented panel by both metrics. Notably, the ecTMB and count estimators suffer weakened results on the augmented panel (the count estimator at regression, ecTMB at classification). While augmenting a gene panel can produce enhanced results, it appears that this (perhaps unsurprisingly) may only be the case when employing an estimator designed in conjunction with the panel selection procedure.

%our proposed workflow in comparison with existing methodology, we began with the Foundation~1 panel, a collection of around 300 genes with a total footprint $\approx$ 0.8Mb. The genes included in the panel have been identified as cancer-relevant or driver genes.

%We augment this gene list via the procedure described in Section \ref{sec:practicalconsiderations} to produce a panel of length of 1Mb. This selection is done by optimising for prediction of tumour indel burden, as this is in general a harder problem than estimation of tumour mutation burden. Based on this selection we can produce linear estimators for \acrshort{tmb} and \acrshort{tib}, and we present results of applying these estimators to an unseen test set of 172 samples in Figure \ref{fig:augpanel}. 

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=6in]{aug_panel_output.png}
% \caption{\textbf{A}: Predictions of \acrshort{tmb} from an augmented 1Mb gene panel. The left figure compares predicted and true values for our estimator, with the shaded red area showing points for which the true value is contained within a 90\% confidence interval of the predicted value. A line of best fit is given in blue, and dashed lines indicate (in black) our classification thresholds and (in red) perfect predictions. The right figure shows the same comparisons for a density-based estimator on non-Foundation genes. \textbf{B}: The same comparisons as in \textbf{A}, but for the prediction of \acrshort{tib}. One sample with particularly high \acrshort{tib} (120) is excluded. \label{fig:augpanel}}
% \end{figure} 

%Since the genes in the Foundation One panel are selected to be cancer drivers, standard methodology would currently dictate that they be excluded from use in prediction. As a comparison therefore, we use density-based estimates calculated from the 0.2Mb of non-Foundation genomic space selected by our augmentation procedure. To make the comparison fair, we allowed the density estimates to be scaled by a linear factor that was learned from the training dataset. We see in Figure \ref{fig:augpanel}\textbf{A} that our estimator does indeed outperform the density-based estimator at prediction, with the additional bonus that we are able to provide error bounds. As would be expected, performance for prediction of \acrshort{tib} is substantially lower (Figure \ref{fig:augpanel}\textbf{B}). However, in this case a density-based estimator is unable to detect meaningful signal.

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=6in]{aug_panel_aucs.png}
% \caption{\jbnote{Change this to PRC} Receiver Operator Characteristic (ROC curves) for \textbf{A}: classification of \acrshort{tmb}-High/Low (threshold 300 mutations), \textbf{B}: classification of \acrshort{tib}-High/Low (threshold of 11 mutations).  \label{fig:augpanelaucs}}
% \end{figure}

% For the augmented panel, we produced ROC plots for classification of \acrshort{tmb} and \acrshort{tib} (shown in Figure \ref{fig:augpanelaucs}). To vary the sensitivity and false positive rate we altered the predictive threshold (while maintaining the threshold for true class labels constant). The red points in Figures \ref{fig:augpanelaucs}\textbf{A} and \ref{fig:augpanelaucs}\textbf{B} refer to classifiers where the predictive threshold is chosen to be the same as the true class threshold. We see excellent overall classification for \acrshort{tmb} and promising performance for \acrshort{tib}.
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=4in]{indel_example.png}
% \caption{For a panel selected to be less than 1Mb in length, taken from the same validation set results as Figure \ref{fig:indelstatsplot}, we directly show predictions vs actual indel burden alongside intercept-free line of best fit. \label{fig:indelexample}}
% \end{figure}





% \begin{figure}[htbp]
% \centering
% \includegraphics[width=6in]{ConfidencePlot.png}
% \caption{ \label{fig:confidenceplot}}
% \end{figure} 

% \section{Theoretical and Simulation Results \label{sec:theoreticalresults}}

% \subsection{Model Misspecification}
% \jbnote{While we don't observe overdispersion (quite the opposite), people do talk about it in the literature. We could write down a) a negative binomial and/or b) a quasi-poisson model and see whether we can derive any guarantees about the performance of our method.} Citation: \citep{martincorena_universal_2017}.


\section{Conclusions \label{sec:conclusion}}
%We present the fitted parameters of the model in Figures \ref{fig:manhat_plot} and \ref{fig:manhat_plot_indel}, where for each we have highlighted the five genes experiencing strongest positive and negative selection. Unsurprisingly, of the five genes with the largest positive coefficients, three (\textit{TP53, KRAS, CDKN2A}) are known tumour suppressors \citep{olivier_tp53_2010, jancik_clinical_2010, foulkes_cdkn2a_1997} and the remaining two (\textit{REG3A, REG1B}) belong to a family with multiple cancer-relevant interactions \citep{chen_four_2019}. Indel mutations in \textit{KRAS} are known to be deleterious for tumour cells, and this in itself is a target for precision gene-editing therapy \citep{lee_selective_2018}. There are many interesting biological interpretations associated with the genes highlighted in Figures~\ref{fig:manhat_plot} and \ref{fig:manhat_plot_indel}; see Section \ref{sec:??}.
 We have proposed a new methodology for designing targeted gene panel which outperforms state-of-the-art procedures for the estimation of tumour mutation burden. We also show promise for estimating other biomarkers such as tumour indel burden, a problem for which no substantial body of work currently exists. These biomarkers are useful for identifying patients who might benefit from immunotherapy, but as of yet techniques for their estimation that are efficient enough to be viable have not been shown to be sufficiently reliable to make the transition to the clinic and a key concern of practitioners remains the confidence they can expect to have in the predictions being made. By grounding our predictive architecture in a generative model of mutation we are able to provide justification for including genes in predictive panels that have previously been excluded, as well as useful practical bounds for the error of our predictions. 
 
 A further advantage of our method is the level of biological interpretation it affords. We were able through optimised sparsity regularisation of our generative model to identify a large subset of genes following approximately the same mutation rate. The notion of a background mutation rate has typically been used as a starting assumption for modelling work, rather than a quantified inference. We were also able to identify genes showing particularly strong positive and negative selection for indel mutations, which in at least one case corresponded directly to an observation at the center of ongoing work on potential cancer therapies. 
 
We see the key contribution of our work not as the performance of our method on the dataset with which we have demonstrated it, but rather its flexibility and broad applicability. There continues to be a large amount of research into immunosurveilling targeted gene panels, with many individual proposal panels presented each year. We have not produced such a panel beyond as a demonstration, but instead have designed our workflow to be as applicable to as broad a range of situations as possible. Our methodology can be used by those with a pre-defined gene panel or by those designing a new panel, either from scratch or augmenting on a starting set of targets. It is flexible in the sense that it can be used to predict biomarkers based on any set of mutation types. This will hopefully allow for further investigation into the utility of tumour indel burden, as well as being robust to changing standards of what genes and mutation types to include when reporting the more established biomarker of tumour mutation burden.
\chapter{Causative Mechanisms of Genomic Instability}

\chapter{Transcripts as Neoantigen Proxies: Expressed Mutation Burden}

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{references.bib}

\appendix

\chapter{High-Dimensional Causal Inference}

\chapter{Supervised Dimension Reduction}

\end{document}
